{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5E36qAPf-8f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NNIA Assignment 9\n",
    "\n",
    "**DEADLINE: 18. 01. 2023 08:00 CET**\n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 3 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2 or 3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv5YtUy8hrYv"
   },
   "source": [
    "## 1 Theory Review (3 pts)\n",
    "\n",
    "\n",
    "\n",
    "Review [chapter 9](https://www.deeplearningbook.org/contents/convnets.html) of the Deep Learning book and the lecture slides and answer the following questions: \n",
    "1. In your own words, how are the concepts of *sparse interactions*, *parameter sharing*, and *equivariant representations* applied in convolutional neural networks? *(1 pt)*\n",
    "1. What kinds of (zero-)padding schemes are there (as decribed in the book)? When should one scheme be chosen over another? (1 pt)\n",
    "1. How do CNNs handle inputs of varrying sizes? (0.5 pts)\n",
    "1. What are learned invariances and what effect do they have on the number of kernel parameters? (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "### 1\n",
    "\n",
    "1. sparse interactions are accomplished by CNNs by making their kernel smaller than the input. This means that, differently from dense NNs, not all input units interact with each output unit. The interactions are reduced. This diminishes memory storage (of fewer parameters),  and the operations needed to calculate the output. The statistical efficiency is improved.\n",
    "\n",
    "parameter sharing is adopted in CNNs when the same set of parameters is shared across different functions in a model. This reduces memory storage.\n",
    "\n",
    "2. There are two main categories of padding. One is referred to by the name valid. This just means no padding. The other type of padding is called same. This means that we want to pad the original input before we convolve it so that the output size is the same size as the input size. A third category is called casual padding and works with the one-dimensional convolutional layers. It is used esoecially in time series analysis. Since a time series is sequential data it helps in adding zeros at the start of the data to enhance the prediction of the values of early time steps.\n",
    "\n",
    "3. With padding.\n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYPfZXJjDeKy"
   },
   "source": [
    "## 2 Output of a convolutional layer (3 pts)\n",
    "The following [blog post](https://www.baeldung.com/cs/convolutional-layer-size) may be helpful for this exercise.  \n",
    "Compute the output of a convonlutional layer given the kernel:  \n",
    "\\begin{array}{|l|l|}\n",
    "\\hline\n",
    "1 & 0 & 1 \\\\ \\hline\n",
    "0 & 1 & 0 \\\\ \\hline\n",
    "1 & 0 & 1 \\\\ \\hline\n",
    "\\end{array}  \n",
    "and the following input RGB image with three values in each cell - one for each channel:  \n",
    "  \n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "1, 2, 3 & 1, 1, 0 & 2, 0, 1 & 1, 1, 0 \\\\ \\hline\n",
    "0, 0, 0 & 1,3,1 & 0, 2, 1 & -1, -2, 0 \\\\ \\hline\n",
    "-1, 0, 0 & 1, 1, 2 & 0, 0, 0 & 1, 1, 0 \\\\ \\hline\n",
    "1, 1, 1 & 1, 0, 2 & 1, 3, -3 & 0, 0, 0 \\\\ \\hline\n",
    "\\end{array}  \n",
    "Use stride = 1 and padding = 0 and state the output dimensionality. What would the output dimensionality be if we used padding = 1?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "### 2  \n",
    "\n",
    "first step of the filter: --> result = 3,5,5\n",
    "1,2,3   0,0,0   2,0,1\n",
    "0,0,0   1,3,1   0,0,0\n",
    "-1,0,0  0,0,0   0,0,0\n",
    "\n",
    "second step of the filter: --> result = 5,7,3\n",
    "1,1,0   0,0,0   1,1,0\n",
    "0,0,0   1,3,1   0,0,0\n",
    "1,1,2   0,0,0   1,1,0\n",
    "\n",
    "third step of the filter: --> result = 3,7,1\n",
    "0,0,0   0,0,0   0,2,1\n",
    "0,0,0   1,1,2   0,0,0\n",
    "1,1,1   0,0,0   1,3,-3\n",
    "\n",
    "fourth step of the filter: --> result = 1,1,3\n",
    "1,3,1   0,0,0   -1,-2,0\n",
    "0,0,0   0,0,0    0,0,0\n",
    "1,0,2   0,0,0    0,0,0\n",
    "\n",
    "end-matrix:\n",
    "3,5,5   5,7,3   \n",
    "3,7,1   1,1,3\n",
    "\n",
    "$ \\begin{bmatrix} 3,5,5 & 5,7,3 \\\\ 3,7,1 & 1,1,3 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RzDiyFDrF0J"
   },
   "source": [
    "## 3 Implementation (4 pts)\n",
    "\n",
    "### Prelude -- Using a validation dataset\n",
    "\n",
    "In this task we also officially introduce the use of a validation set in a homework assignment.\n",
    "\n",
    "A validation set allows you to due some further fine-tuning of your hyperparameters on a set of data that was not used for training. This has two purposes: 1) testing the generalization capabilities of your model and 2) verify that your model can handle unseen data as this may come in formats that you were not specifically expecting.\n",
    "\n",
    "Take a look at these articles to gain more insight into why we use validation sets in the world of Deep Learning:\n",
    "\n",
    "* [Why use both validation set and test set?](https://datascience.stackexchange.com/a/18346)\n",
    "* [Why Do We Need a Validation Set in Addition to Training and Test Sets?](https://towardsdatascience.com/why-do-we-need-a-validation-set-in-addition-to-training-and-test-sets-5cf4a65550e0)\n",
    "\n",
    "### Implementation task\n",
    "In this exercise, we will continue to work with [the PyTorch Datasets Class](https://pytorch.org/vision/stable/datasets.html) to obtain\n",
    "[the CIFAR100 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). Instead of the simple neural network from the previous assignment, we are going to implement a convolution neural network (CNN) model to classify the images in this dataset into their proper classes.\n",
    "\n",
    "Your CNN model will have the following architecture:\n",
    "\n",
    "\n",
    "* It will have five convolution blocks. \n",
    "* Each block consists of the *convolution*, *max pooling* and *ReLU* operation in that order. \n",
    "* We will use $3\\times3$ kernels in all convolutional layers. Set the padding and stride of the convolutional layers so that they **maintain** the spatial dimensions. \n",
    "* Max pooling operations are done with $2\\times2$ kernels, with a stride of 2, thereby **halving** the spatial resolution each time. \n",
    "* Finally, five stacking these five blocks leads to a $512\\times1\\times1$ feature map. \n",
    "* Classification is achieved using a fully connected layer. \n",
    "\n",
    "Implement the class *ConvNet* to define the model described. The ConvNet model takes $32\\times32$ color images as inputs and has 5 hidden layers with 128, 512, 512, 512, 512 filters, and produces a 100-class classification. We will train the convolutional neural network on the CIFAR-100 dataset. Feel free to incorporate drop-put, batch normalization, and early stopping if desired. Evaluate your trained model on the test set and report your findings.\n",
    "\n",
    "For loss, you can use cross entropy loss and for optimization, you can use the Adam optimizer with the learning rate of `2e-3` and weight decay of $0.001$. \n",
    "       \n",
    "**Note**: To speed up trainining on the entire dataset, you may want an access to a GPU (CPU runtime > 10 hrs vs < 5 mins GPU). We recommend you make use of [Google Colab](https://colab.research.google.com/?utm_source=scs-index).  \n",
    "If you are having trouble loading the dataset [this post](https://stackoverflow.com/questions/71263622/sslcertverificationerror-when-downloading-pytorch-datasets-via-torchvision) on stackoverflow may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG5y_d8vrPeR"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKn5Jwr1Qr78"
   },
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WilMrkWCUCdw"
   },
   "outputs": [],
   "source": [
    "# We recommend using a GPU for this task, if not available a CPU will be used\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DjoaSmQrQO7"
   },
   "outputs": [],
   "source": [
    "# Feel free to modify these variables how you deem fit,\n",
    "# as long as you are still following the above instructions\n",
    "# (e.g. you respect the proposed network architecture)\n",
    "LR = 2e-3\n",
    "REG = 0.001\n",
    "INPUT_SIZE = 3\n",
    "NUM_CLASSES = 100\n",
    "HIDDEN_SIZE = [128, 512, 512, 512, 512, 512]\n",
    "NUM_EPOCHS = ...\n",
    "BATCH_SIZE = ...\n",
    "LR_DECAY = ...\n",
    "TRAINING_SIZE = ...\n",
    "\n",
    "# Percentage of the training data to be used as validation data\n",
    "VAL_SIZE = ...\n",
    "DROP_OUT = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggC1IvHORjQW"
   },
   "outputs": [],
   "source": [
    "def get_cifar100_dataset(val_size=VAL_SIZE, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Load and transform the CIFAR100 dataset. Make Validation set. Create dataloaders for\n",
    "    train, test, validation sets.\n",
    "\n",
    "    NOTES:\n",
    "    1. DO NOT CHANGE THE CODE IN THIS FUNCTION. YOU MAY CHANGE THE BATCH_SIZE PARAM IF NEEDED.\n",
    "    2. If you get an error related `num_workers`, you may change that parameter to a different value.\n",
    "\n",
    "    :param val_size: size of the validation partition\n",
    "    :param batch_size: number of samples in a batch\n",
    "    :return: three data loaders and the set of possible classes\n",
    "    \"\"\"\n",
    "\n",
    "    # the datasets.CIFAR getitem actually returns img in PIL format\n",
    "    # no need to get to Tensor since we're working with our own model and not PyTorch\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.24703233, 0.24348505, 0.26158768))\n",
    "                                    ])\n",
    "\n",
    "    # Load the train_set and test_set from PyTorch, transform each sample to a flattened array\n",
    "    train_set = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "    test_set = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "    classes = train_set.classes\n",
    "\n",
    "    # Split data and define train_loader, test_loader, val_loader\n",
    "    val_size = int(len(train_set) * val_size)\n",
    "    train_size = len(train_set) - val_size\n",
    "    train_set, val_set = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=len(test_set),\n",
    "                                              shuffle=False, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=val_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader, test_loader, val_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "cda440171b7f443794db7daf27cde952",
      "edf5bd74d5af4332909a8e97bee31642",
      "0274757e96e642b89276b29348cf61ae",
      "1279f9bf1cf44e1ebaaecc0bf0e5bf25",
      "c6405241aebe4dd3a6faf8e90e6264dd",
      "f8149852b55041db9c149abd547f7ef6",
      "d20f6d1e5af140acac4cea3bf06cf644",
      "c82ec9b4b12c4197a5db36f200e9d04f",
      "b14bb8aaf11c44e8bd6bbaff08875502",
      "3ad90b1fd6d74a47877785a0405bba48",
      "cc4223bb20fa433489a6fb39aa5d6e51"
     ]
    },
    "id": "9h5JRzcPSyW3",
    "outputId": "36e74856-c149-4db3-93f5-b63d525276da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, val_loader, classes = get_cifar100_dataset(val_size=0.1, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0RR9PHXSrUM"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the class `ConvNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lw27Fl_SsK3"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the forward pass computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdVC830eStN2"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lc5MV9eMQ1d5"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement the evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvGTnsyYGGru"
   },
   "source": [
    "## Bonus CNNs and NLP\n",
    "Research the web and discuss how CNNs can be applied to NLP tasks:   \n",
    "1. How is the input defined?\n",
    "2. What advantages do CNNs have over fully connected NNs?  \n",
    "3. For an RGB image input there are normally three channels - one for each color. What different channels can we consider given a language input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0274757e96e642b89276b29348cf61ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c82ec9b4b12c4197a5db36f200e9d04f",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b14bb8aaf11c44e8bd6bbaff08875502",
      "value": 169001437
     }
    },
    "1279f9bf1cf44e1ebaaecc0bf0e5bf25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ad90b1fd6d74a47877785a0405bba48",
      "placeholder": "​",
      "style": "IPY_MODEL_cc4223bb20fa433489a6fb39aa5d6e51",
      "value": " 169001437/169001437 [00:03&lt;00:00, 53499228.59it/s]"
     }
    },
    "3ad90b1fd6d74a47877785a0405bba48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b14bb8aaf11c44e8bd6bbaff08875502": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c6405241aebe4dd3a6faf8e90e6264dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c82ec9b4b12c4197a5db36f200e9d04f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc4223bb20fa433489a6fb39aa5d6e51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cda440171b7f443794db7daf27cde952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_edf5bd74d5af4332909a8e97bee31642",
       "IPY_MODEL_0274757e96e642b89276b29348cf61ae",
       "IPY_MODEL_1279f9bf1cf44e1ebaaecc0bf0e5bf25"
      ],
      "layout": "IPY_MODEL_c6405241aebe4dd3a6faf8e90e6264dd"
     }
    },
    "d20f6d1e5af140acac4cea3bf06cf644": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edf5bd74d5af4332909a8e97bee31642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8149852b55041db9c149abd547f7ef6",
      "placeholder": "​",
      "style": "IPY_MODEL_d20f6d1e5af140acac4cea3bf06cf644",
      "value": "100%"
     }
    },
    "f8149852b55041db9c149abd547f7ef6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
