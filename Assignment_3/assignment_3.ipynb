{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT_S_io_85jk"
   },
   "source": [
    "# NNIA Assignment 3\n",
    "\n",
    "**DEADLINE: 30. 11. 2021 08:00 CET**  \n",
    "Submissions past the deadline (08.01) will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx): \n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 3 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person: \n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/l9so16qqvk34hu)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2/3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required. \n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2_Name3_id3.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2_Name3_id3.ipynb`. This is **very important** for our internal organization.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Ur365126b3"
   },
   "source": [
    "# Regressions and Model Evaluation\n",
    "\n",
    "The objectives of the exercises in this assignment are:\n",
    "\n",
    "*   to develop an intuitive understanding of a regression model's trainable parameters\n",
    "*   to have a better understanding of PCA in practice \n",
    "*   to familiarize ourselves with how to fit various regression models\n",
    "*   to learn about various evaluation metrics and their characteristics\n",
    "\n",
    "Before going ahead with the exercises, recall the following information about Simple and Multiple Linear Regressions from the lecture slides.\n",
    "\n",
    "\n",
    "*   A *Simple Linear Regression* model predicts a quantitative response $y$ given a single predictor variable $x$ using the best fitting line $y \\approx mx + b$ for the observed data.\n",
    "\n",
    "*   In *Multiple Linear Regression*, the model predicts a quantitative response $y$ given multiple predictor variables by fitting a model $y \\approx w_{0} +w_{1}x_{1} + w_{2}x_{2} +... + w_{n}x_{n}$ to the observed data.\n",
    "\n",
    "*   An ideal model minizes the average squared distance between estimated response of the *i*−th sample $\\hat{y}^{train}$ and actual response $y^{train}$ of the *i*−th\n",
    "sample:\n",
    "\n",
    "\n",
    "$$MSE_{train} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_{i}^{train} - y_{i}^{train})^{2} $$\n",
    "\n",
    "*   To minimize $MSE_{train}$, we can set the gradient w.r.t. $w$ to $0$, solving for the weights or parameters $w$:\n",
    "\n",
    "$$w = (X_{train}^{T}X_{train})^{-1}X_{train}^{T}y_{train}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression Manually (2.5 points)\n",
    "\n",
    "In this task we ask you to solve linear regression manually to gain better understanding of internals of sklearn method, that you are allowed to use for next tasks.\n",
    "\n",
    "1.1 Implement linear regression manually following the instructions above (1.5 points)\n",
    "\n",
    "1.2 How does RMSE change depending on the test size? What does it show? (0.5 points)\n",
    "\n",
    "1.3 What makes this approach inefficient? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 <font color=\"red\">Done</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52.382356168443614, 54.576536424875066, 59.4582946089298)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def manual_linear_regression(test_size: int = 0.1, seed: int = 42):\n",
    "    # Load the dataset.\n",
    "    dataset = load_diabetes()\n",
    "\n",
    "    # The input data are in `dataset.data`, targets are in `dataset.target`.\n",
    "    #print(dataset.data, dataset.target)\n",
    "    # data.shape (442,10)\n",
    "    # target.shape (442,)\n",
    "    in_data = dataset.data\n",
    "    target_data = dataset.target\n",
    "    #print('Input:\\n',in_data,'\\n')\n",
    "    #print('Target:\\n',target_data,'\\n')\n",
    "\n",
    "    # If you want to learn more about the dataset, you can print some information\n",
    "    # about it using `print(dataset.DESCR)`.\n",
    "    #print(dataset.DESCR)\n",
    "\n",
    "    # DONE: Append a constant feature with value 1 to the end of every input data.\n",
    "    # Then we do not need to explicitly represent bias - it becomes the last weight.\n",
    "    bias = np.ones((442,1))\n",
    "    in_data = np.append(in_data,bias,axis=1) # Shape: (442,11)\n",
    "    #print(in_data)\n",
    "    #print(in_data.shape)\n",
    "    \n",
    "    # DONE: Split the dataset into a train set and a test set.\n",
    "    # Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "    # arguments `test_size, random_state=seed`.\n",
    "    X_train, X_test, y_train, y_test =train_test_split(in_data,target_data,test_size=test_size,random_state=seed)\n",
    "    #print('X_train',X_train.shape)\n",
    "    #print('y_train',y_train.shape)\n",
    "    #print('X_test',X_test.shape)\n",
    "    #print('y_test',y_test.shape)\n",
    "\n",
    "    # DONE: Solve the linear regression using the algorithm from the lecture,\n",
    "    # explicitly computing the matrix inverse (using `np.linalg.inv`).\n",
    "    # linear least squares\n",
    "    w = inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "    #print('w',w) # Shape (11,)\n",
    "    \n",
    "    # DONE: Predict target values on the test set.\n",
    "    y_hat = X_test.dot(w)\n",
    "    #print(y_hat.shape)\n",
    "    \n",
    "    # DONE: Manually compute root mean square error on the test set predictions.\n",
    "    #rmse = mean_squared_error(y_test,y_hat,squared=False) # squared=False returns rmse instead of mse\n",
    "    rmse = np.sqrt(((y_hat-y_test)**2).mean())\n",
    "\n",
    "    return rmse\n",
    "\n",
    "manual_linear_regression(), manual_linear_regression(test_size=0.5), manual_linear_regression(test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 <font color=\"red\">Done</font>\n",
    "\n",
    "Increasing the test size increases the error. This seems intuitive, because while increasing the test size we are simultaneously decreasing the training size. Therefore the model has seen less variance during training for all variables, while at the same time having to predict more data with more variance during testing.\n",
    "\n",
    "### 1.3 <font color=\"red\">Done</font>\n",
    "\n",
    "Acquiring the parameters for w directly is computationally expensive and numerically unstable. The predictors (the columns in X) may also not be colinear, otherwise this approach won't work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Efficient Linear Regression (3.5 points)\n",
    "\n",
    "For the other tasks, we will be working with the [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) unless otherwise indicated.\n",
    "Recall that there are 8 features that influence the housing prices in California according to this dataset. \n",
    "\n",
    "Although we want to consider as many predictive features as we can in our model, doing so may not necessarily be practical or desirable. Let's consider an assumption that Linear Regression is an algorithm that takes one extra hour to compute for every input feature (it does not but some models may work like this). Therefore, for the sake of efficiency, we want to limit the number of features in the dataset to 3.\n",
    "\n",
    "## 2.1 Dimensions $8 \\rightarrow 3$ (2 points)\n",
    "\n",
    "To reduce the 8 features to 3, we need to find out which features we should keep and which ones we can ignore. Implement the following two methods to find out:\n",
    "\n",
    "1. Try all subsets of size 3 of all the features and report which subset results in a Linear Regression model with the lowest MSE. (1 points)\n",
    "2. Perform PCA to 3 dimensions (components) and fit a Linear Regression using these 3 features. Report the 3 features selected by PCA and the MSE. (0.5 points)\n",
    "3. Compare the approaches and name one advantage of each over the other method. Comment on any insight you gain about the relationship between housing prices and the selected features in the dataset. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWUfkmjdIXNS"
   },
   "source": [
    "### 2.1.1 <font color=\"red\">Done</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(['MedInc', 'HouseAge', 'AveRooms'], 0.6496608827746702)\n",
      "(['MedInc', 'HouseAge', 'AveBedrms'], 0.6534321198005493)\n",
      "(['MedInc', 'HouseAge', 'Population'], 0.6524644655188572)\n",
      "(['MedInc', 'HouseAge', 'AveOccup'], 0.6515442973231507)\n",
      "(['MedInc', 'HouseAge', 'Latitude'], 0.6428821991178675)\n",
      "(['MedInc', 'HouseAge', 'Longitude'], 0.6533445563317436)\n",
      "(['MedInc', 'AveRooms', 'AveBedrms'], 0.6541749417574403)\n",
      "(['MedInc', 'AveRooms', 'Population'], 0.6916413203415843)\n",
      "(['MedInc', 'AveRooms', 'AveOccup'], 0.6913124252392123)\n",
      "(['MedInc', 'AveRooms', 'Latitude'], 0.6847831073780369)\n",
      "(['MedInc', 'AveRooms', 'Longitude'], 0.6913296359470075)\n",
      "(['MedInc', 'AveBedrms', 'Population'], 0.700042936976814)\n",
      "(['MedInc', 'AveBedrms', 'AveOccup'], 0.6993184146572058)\n",
      "(['MedInc', 'AveBedrms', 'Latitude'], 0.6904532907506266)\n",
      "(['MedInc', 'AveBedrms', 'Longitude'], 0.6994333086808937)\n",
      "(['MedInc', 'Population', 'AveOccup'], 0.6984774384996097)\n",
      "(['MedInc', 'Population', 'Latitude'], 0.6885412118288013)\n",
      "(['MedInc', 'Population', 'Longitude'], 0.6986477508471665)\n",
      "(['MedInc', 'AveOccup', 'Latitude'], 0.6887021644793743)\n",
      "(['MedInc', 'AveOccup', 'Longitude'], 0.6976707142394544)\n",
      "(['MedInc', 'Latitude', 'Longitude'], 0.5535797036700205)\n",
      "(['HouseAge', 'AveRooms', 'AveBedrms'], 1.1192510722614388)\n",
      "(['HouseAge', 'AveRooms', 'Population'], 1.2770737020527865)\n",
      "(['HouseAge', 'AveRooms', 'AveOccup'], 1.2773372229220599)\n",
      "(['HouseAge', 'AveRooms', 'Latitude'], 1.2419147806536637)\n",
      "(['HouseAge', 'AveRooms', 'Longitude'], 1.2771661251105066)\n",
      "(['HouseAge', 'AveBedrms', 'Population'], 1.3146956607422995)\n",
      "(['HouseAge', 'AveBedrms', 'AveOccup'], 1.3138560722007544)\n",
      "(['HouseAge', 'AveBedrms', 'Latitude'], 1.2874913540984447)\n",
      "(['HouseAge', 'AveBedrms', 'Longitude'], 1.3131210582907646)\n",
      "(['HouseAge', 'Population', 'AveOccup'], 1.3157480618103892)\n",
      "(['HouseAge', 'Population', 'Latitude'], 1.2884525582887938)\n",
      "(['HouseAge', 'Population', 'Longitude'], 1.3149688569260363)\n",
      "(['HouseAge', 'AveOccup', 'Latitude'], 1.28774452725297)\n",
      "(['HouseAge', 'AveOccup', 'Longitude'], 1.3142558876647223)\n",
      "(['HouseAge', 'Latitude', 'Longitude'], 1.0084862353220336)\n",
      "(['AveRooms', 'AveBedrms', 'Population'], 1.1547664504188921)\n",
      "(['AveRooms', 'AveBedrms', 'AveOccup'], 1.1543580518967627)\n",
      "(['AveRooms', 'AveBedrms', 'Latitude'], 1.1147386910435753)\n",
      "(['AveRooms', 'AveBedrms', 'Longitude'], 1.1546647240670624)\n",
      "(['AveRooms', 'Population', 'AveOccup'], 1.2999064536755387)\n",
      "(['AveRooms', 'Population', 'Latitude'], 1.2649653917081516)\n",
      "(['AveRooms', 'Population', 'Longitude'], 1.2983565421284409)\n",
      "(['AveRooms', 'AveOccup', 'Latitude'], 1.2655124838264888)\n",
      "(['AveRooms', 'AveOccup', 'Longitude'], 1.2977823651409819)\n",
      "(['AveRooms', 'Latitude', 'Longitude'], 0.9181032302110539)\n",
      "(['AveBedrms', 'Population', 'AveOccup'], 1.326964045248384)\n",
      "(['AveBedrms', 'Population', 'Latitude'], 1.2996391344174107)\n",
      "(['AveBedrms', 'Population', 'Longitude'], 1.325182322575005)\n",
      "(['AveBedrms', 'AveOccup', 'Latitude'], 1.301336976550302)\n",
      "(['AveBedrms', 'AveOccup', 'Longitude'], 1.3251468125685064)\n",
      "(['AveBedrms', 'Latitude', 'Longitude'], 1.003802616831516)\n",
      "(['Population', 'AveOccup', 'Latitude'], 1.3011215579685924)\n",
      "(['Population', 'AveOccup', 'Longitude'], 1.3275351978333403)\n",
      "(['Population', 'Latitude', 'Longitude'], 1.006453327351388)\n",
      "(['AveOccup', 'Latitude', 'Longitude'], 1.0083423918775416)\n",
      "\n",
      "\n",
      "Min error set:  ['MedInc', 'Latitude', 'Longitude'] \n",
      " Min error:  0.5535797036700205\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from sklearn.datasets import fetch_california_housing\n",
    "#housing = fetch_california_housing()\n",
    "from Solution_A3 import lin_reg_subsets\n",
    "lin_reg_subsets()\n",
    "\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be MSEs and feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIelNBj9IdbO"
   },
   "source": [
    "### 2.1.2 <font color=\"red\">Done</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['PC 1 Latitude', 'PC 2 AveRooms', 'PC 3 Population'], 1.2929760346153332)\n"
     ]
    }
   ],
   "source": [
    "from Solution_A3 import house_pca_reg\n",
    "\n",
    "house_pca_reg()\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be MSE and feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_DByZGSdOEg"
   },
   "source": [
    "### 2.1.3 <font color=\"red\">Done</font>\n",
    "\n",
    "One advantage of the subset approach over PCA is, that we get a much lower error (0.55 compared to 1.29). Moreover, we retain the original features, which are human-readable and explainable. The disadvantage however is, that we need to test all possible subset combinations of 3 features here to find the minimal error. For 8 variables, we have to test for 56 combinations, for 9 variables there are already 84 combinations, so this number grows very fast. PCA on the other hand remains computationally feasible. But the principal components that are extracted are difficult to interpret. Our approach here in naming the components was to select the strongest correlation with all of the 8 features.\n",
    "\n",
    "Interestingly, the first principal component has the strongest relationship to the variable \"Latitude\", which is also part of the subset that produced the minimal error value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-pS81_N4RJF"
   },
   "source": [
    "## 2.2 Dimensions $8 \\rightarrow 1$ (1.5 points)\n",
    "\n",
    "Having to visualize the data across multiple dimensions can be cumbsersome. Let's perform the same task as in 2.1 but this time consider only one feature (both a subset of the 8 features and PCA with 1 component). This way it will be easier to visualize the relationaship of your predictive and target variables. Of course, you still want to select the feature that will result in the best performing model.\n",
    "\n",
    "Your output should include:\n",
    "\n",
    "1.   The respective MSEs (only the lowest MSE for the subset is fine.)\n",
    "2.   Make [a scatter plot](https://en.wikipedia.org/wiki/Scatter_plot) of the data with prices on the $y$ axis and the single feature/principal component on the $x$ axis. In the plot, also include a line as defined by the Linear Regression. (Make sure you don't forget to set the correct slope and y-intercept (constant))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1636738522928,
     "user": {
      "displayName": "Vilém Zouhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjqsASJYVkME5c30t4iFtrEDz__wtyoHv63qsWl4A=s64",
      "userId": "15042577288802340958"
     },
     "user_tz": -60
    },
    "id": "rbBzhK6f42J2",
    "outputId": "b0ef4961-7287-493b-e8b8-aacd7dd83209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# from solution import ....\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be the MSEs, feauture names, and plots (make sure to label your plots!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxMm8kFf_DYF"
   },
   "source": [
    "## 3 Manual Regression (4 points)\n",
    "\n",
    "Now that you are a bit more familiar with the data and the features. This exercise aims to develop some intuition behind regressions by manually adjusting the parameters (coefficients and intercept) in the model. The functions below all perform regression (predicting a real value) but they are far from perfect. Your goal is to improve the four functions from the initial ones in the `code cells` as follows:\n",
    "\n",
    "1. `hand_base` should serve as a baseline. The constraint is that it should only return a *single (constant) number* for all values. In other words, this is a model with no adjustable parameters. However, for the dataset there exists a unique value that minimizes the Mean Squared Error (MSE). Which one is it? (0.5 points) \n",
    "2. `hand_linear` should be a reasonable *linear* function that utilizes the input feature(s). Note that it should be strictly linear, that is in the form $\\sum \\lambda_i x_i+\\lambda_{const}$ where $\\lambda_k$ and $\\lambda_{const}$ are the coefficients and intercept that you can estimate from the given data by *trial and error*. Your estimates should be reasonable, i.e. definitely better than `hand_base`. Do this exercise before proceeding to the next function where you will obtain the coefficients and intercepts from fitting a Linear Regression model using *sklearn*. We will award full points based on any justified solution that's better than `hand_base`. Make sure that you read what the features mean and argue why you chose the specific formula. (1 point) (Note: we are *not* asking you to compute the coefficients and intercept, but rather play around with adjusting the coefficients and intercept manually to arrive at your best estimate.)\n",
    "3. `auto_linear`, obtain the coefficients and intercept from fitting a Linear Regression model using `sklearn`.\n",
    "(Consult [sklearn Linear Regression Documention](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on how to obtain the model's coefficients and intercept.) (1 point)\n",
    "\n",
    "4. `hand_complex` does not have any restriction on the content of the function. It can contain polynomial relationships (e.g. `x[0]*x[0]`), `if-else` statements, etc.) Now that you have both your hand crafted model and the one from `sklearn`, improve upon either of the models (or you can start with the parameters in the `auto_linear` model) so that the performance of the `hand_complex` is better than `auto_linear`.\n",
    "What are the disadvantages of this more complex approach apart from the difficulty of creating it? (Hint: think about unseen data.)\n",
    "\n",
    "Always comment on what led you to select the specific values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13EgQJtJdu7w"
   },
   "source": [
    "## 3 <font color=\"red\">To Do</font>\n",
    "\n",
    "Modify the functions in the `code cell` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1636912009088,
     "user": {
      "displayName": "Noonsky Noon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14548880660870589126"
     },
     "user_tz": 480
    },
    "id": "yo0nFWosRS_T",
    "outputId": "c0c3d622-4009-431d-a93a-dfad3701ef95"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Features\", housing.feature_names)\n",
    "\n",
    "def hand_base(_x):\n",
    "  # TODO: choose better single value\n",
    "  return 0\n",
    "\n",
    "def hand_linear(x):\n",
    "  # TODO: make me better but only linearly\n",
    "  return 2*x[0]-0.5*x[1]-0.1\n",
    "\n",
    "# TODO:\n",
    "# 1. Fit LinearRegression\n",
    "# 2. Report training MSE\n",
    "# 3. Examine the coefficients and intercept and use them for the `auto_linear` function\n",
    "# <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>\n",
    "\n",
    "def auto_linear(x):\n",
    "  # TODO: use coefficients from your linear regression\n",
    "  return 2*x[0]-0.5*x[1]-0.1\n",
    "\n",
    "def hand_complex(x):\n",
    "  # TODO: make me better than the auto_linear function\n",
    "  if x[0] < 0.5:\n",
    "    return 0.1*x[1]\n",
    "  else:\n",
    "    return 0.2*x[1]\n",
    "\n",
    "print(f\"MSE Hand-Base: {mse(housing_y, [hand_base(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Hand-Linear: {mse(housing_y, [hand_linear(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Auto-LR: {mse(housing_y, [auto_linear(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Hand-Complex: {mse(housing_y, [hand_complex(x) for x in housing_x]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Pu7huYRyoW"
   },
   "source": [
    "# Bonus. Polynomial Regression and Overfitting (1 point):\n",
    "\n",
    "Find out how incorporating more features affects our model on the [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n",
    "\n",
    "1. Transform the feature space using polynomial features: <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html> (hint: make use of the Pipeline class) and run a regression model on top of it. Use degrees 1, 2, 3 and 4. \n",
    "\n",
    "2. Make a scatter plot with polynomial degree on the x-axis and training MSE on the y axis. What is an essential caveat to expanding the original feature space like this? (Hint: Think of unseen data again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MdJPr9lDtUd"
   },
   "source": [
    "## Bonus: <font color=\"red\">To Do</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 2112,
     "status": "ok",
     "timestamp": 1636738647952,
     "user": {
      "displayName": "Vilém Zouhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjqsASJYVkME5c30t4iFtrEDz__wtyoHv63qsWl4A=s64",
      "userId": "15042577288802340958"
     },
     "user_tz": -60
    },
    "id": "ccreCT1RDscQ",
    "outputId": "9282782f-00d0-4e8c-f891-823a434c6c85"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  \n",
    "# from solution import ....\n",
    "# import your function from your .py file here and run this cell when you're done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "revised_assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d234eaca08f81c86c57ba3fab09f675b7f794e918ca74eabd6cec66c7f752238"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
