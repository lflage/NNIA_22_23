{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT_S_io_85jk"
   },
   "source": [
    "# NNIA Assignment 3\n",
    "\n",
    "**DEADLINE: 30. 11. 2021 08:00 CET**  \n",
    "Submissions past the deadline (08.01) will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx): \n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 3 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person: \n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/l9so16qqvk34hu)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2/3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required. \n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2_Name3_id3.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2_Name3_id3.ipynb`. This is **very important** for our internal organization.\n",
    "\n",
    "<font color=\"red\">Failure to follow the above instructions will result in point penalties at the discretion of the instructors.</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-Ur365126b3"
   },
   "source": [
    "# Regressions and Model Evaluation\n",
    "\n",
    "The objectives of the exercises in this assignment are:\n",
    "\n",
    "*   to develop an intuitive understanding of a regression model's trainable parameters\n",
    "*   to have a better understanding of PCA in practice \n",
    "*   to familiarize ourselves with how to fit various regression models\n",
    "*   to learn about various evaluation metrics and their characteristics\n",
    "\n",
    "Before going ahead with the exercises, recall the following information about Simple and Multiple Linear Regressions from the lecture slides.\n",
    "\n",
    "\n",
    "*   A *Simple Linear Regression* model predicts a quantitative response $y$ given a single predictor variable $x$ using the best fitting line $y \\approx mx + b$ for the observed data.\n",
    "\n",
    "*   In *Multiple Linear Regression*, the model predicts a quantitative response $y$ given multiple predictor variables by fitting a model $y \\approx w_{0} +w_{1}x_{1} + w_{2}x_{2} +... + w_{n}x_{n}$ to the observed data.\n",
    "\n",
    "*   An ideal model minizes the average squared distance between estimated response of the *i*−th sample $\\hat{y}^{train}$ and actual response $y^{train}$ of the *i*−th\n",
    "sample:\n",
    "\n",
    "\n",
    "$$MSE_{train} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_{i}^{train} - y_{i}^{train})^{2} $$\n",
    "\n",
    "*   To minimize $MSE_{train}$, we can set the gradient w.r.t. $w$ to $0$, solving for the weights or parameters $w$:\n",
    "\n",
    "$$w = (X_{train}^{T}X_{train})^{-1}X_{train}^{T}y_{train}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Regression Manually (2.5 points)\n",
    "\n",
    "In this task we ask you to solve linear regression manually to gain better understanding of internals of sklearn method, that you are allowed to use for next tasks.\n",
    "\n",
    "1.1 Implement linear regression manually following the instructions above (1.5 points)\n",
    "\n",
    "1.2 How does RMSE change depending on the test size? What does it show? (0.5 points)\n",
    "\n",
    "1.3 What makes this approach inefficient? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 <font color=\"red\">To Do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(161.54116869178804, 162.84970465778542, 187.8102369491661)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1 Implement linear regression manually\n",
    "'''\n",
    "x as feature vector, i.e x = [x_1, x_2, …., x_n],\n",
    "y as response vector, i.e y = [y_1, y_2, …., y_n]\n",
    "for n observations (in above example, n=10).\n",
    "'''\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "def manual_linear_regression(test_size: int = 0.1, seed: int = 42):\n",
    "    # Load the dataset.\n",
    "    dataset = load_diabetes()\n",
    "    #print(dataset.data.shape)\n",
    "    input_data = dataset.data\n",
    "    output_data = dataset.target\n",
    "    #print(dataset.data)\n",
    "    #print(dataset.target)\n",
    "    #print(dataset.DESCR)\n",
    "    data_df = pd.DataFrame(input_data,columns=dataset.feature_names)\n",
    "    data_df['target'] = output_data\n",
    "    \n",
    "\n",
    "    # Append a constant feature with value 1 to the end of every input data.\n",
    "    # Then we do not need to explicitly represent bias - it becomes the last weight.\n",
    "    bias = np.ones((442,1))\n",
    "    input_data = np.append(input_data,bias,axis=1) # Shape: (442,11)\n",
    "    #for data in input_data:\n",
    "    #    data = np.append(input_data,int(1)) \n",
    "    #    biased_input = np.append(biased_input,data)\n",
    "    #print(biased_input.shape)\n",
    "    \n",
    "    # Split the dataset into a train set and a test set.\n",
    "    # if I use test_split without argument I get an error --> I don't understand the instructions\n",
    "    #independent variables / explanatory variables\n",
    "    X = data_df.drop(labels='target', axis=1)  #axis=1 means we drop data by column.\n",
    "\n",
    "    #dependent variable / response / target variable.\n",
    "    y = data_df['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=seed)\n",
    "\n",
    "    #print(X_train.shape)\n",
    "    #print(y_train.shape)\n",
    "    #print(X_test.shape)\n",
    "    #print(y_test.shape)\n",
    "\n",
    "    # Solve the linear regression using the algorithm from the lecture,\n",
    "    # explicitly computing the matrix inverse (using `np.linalg.inv`).\n",
    "    # w = (X_train)T.X_train)^-1.X_trainT.y_train\n",
    "    w = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "    \n",
    "    # Predict target values on the test set\n",
    "    y_result = X.dot(w)\n",
    "\n",
    "    #  Manually compute root mean square error on the test set predictions\n",
    "    rmse = 0\n",
    "    mse = np.square(np.subtract(y,y_result)).mean() \n",
    " \n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "manual_linear_regression(), manual_linear_regression(test_size=0.5), manual_linear_regression(test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 <font color=\"red\">Done</font>\n",
    "\n",
    "I don't see any change\n",
    "\n",
    "### 1.3 <font color=\"red\">To Do</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Efficient Linear Regression (3.5 points)\n",
    "\n",
    "For the other tasks, we will be working with the [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) unless otherwise indicated.\n",
    "Recall that there are 8 features that influence the housing prices in California according to this dataset. \n",
    "\n",
    "Although we want to consider as many predictive features as we can in our model, doing so may not necessarily be practical or desirable. Let's consider an assumption that Linear Regression is an algorithm that takes one extra hour to compute for every input feature (it does not but some models may work like this). Therefore, for the sake of efficiency, we want to limit the number of features in the dataset to 3.\n",
    "\n",
    "## 2.1 Dimensions $8 \\rightarrow 3$ (2 points)\n",
    "\n",
    "To reduce the 8 features to 3, we need to find out which features we should keep and which ones we can ignore. Implement the following two methods to find out:\n",
    "\n",
    "1. Try all subsets of size 3 of all the features and report which subset results in a Linear Regression model with the lowest MSE. (1 points)\n",
    "2. Perform PCA to 3 dimensions (components) and fit a Linear Regression using these 3 features. Report the 3 features selected by PCA and the MSE. (0.5 points)\n",
    "3. Compare the approaches and name one advantage of each over the other method. Comment on any insight you gain about the relationship between housing prices and the selected features in the dataset. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWUfkmjdIXNS"
   },
   "source": [
    "### 2.1.1 <font color=\"red\">To Do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subset {'MedInc', 'AveRooms', 'HouseAge'} results in a Linear Regression model with the lowest MSE.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "input_data = housing.data\n",
    "output_data = housing.target\n",
    "feature_names = housing.feature_names\n",
    "housing_df = pd.DataFrame(input_data,columns=feature_names)\n",
    "housing_df['target'] = output_data\n",
    "\n",
    "# Find all subsets len=3 in feature_names\n",
    "\n",
    "def findsubsets(input_set, size):\n",
    "    return [set(i) for i in itertools.combinations(input_set, size)]\n",
    "     \n",
    "input_set = set(feature_names)\n",
    "size = 3\n",
    " \n",
    "subsets=findsubsets(input_set, size)\n",
    "\n",
    "\n",
    "def manual_linear_regression_subset(dataset, test_size: int = 0.1, seed: int = 42):\n",
    "    results=[]\n",
    "    input_data = dataset.data\n",
    "    output_data = dataset.target\n",
    "    data_df = pd.DataFrame(input_data,columns=housing.feature_names)\n",
    "    data_df['target'] = output_data\n",
    "    #print(dataset.data)\n",
    "    #print(dataset.target)\n",
    "    #print(dataset.DESCR)\n",
    "    for subset in subsets:\n",
    "        df_subset=pd.DataFrame(data_df[subset],columns=subset)\n",
    "        #df_subset['target'] = output_data\n",
    "        #print(df_subset.shape)\n",
    "        \n",
    "        # Append a constant feature with value 1 to the end of every input data ?\n",
    "\n",
    "        # Split the dataset into a train set and a test set.\n",
    "        # if I use test_split without argument I get an error --> I don't understand the instructions\n",
    "        #independent variables / explanatory variables\n",
    "        X = df_subset.to_numpy()\n",
    "\n",
    "        #dependent variable / response / target variable.\n",
    "        y = output_data\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=seed)\n",
    "\n",
    "        #print(X_train.shape)\n",
    "        #print(y_train.shape)\n",
    "        #print(X_test.shape)\n",
    "        #print(y_test.shape)\n",
    "\n",
    "        # Solve the linear regression using the algorithm from the lecture,\n",
    "        # explicitly computing the matrix inverse (using `np.linalg.inv`).\n",
    "        # w = (X_train)T.X_train)^-1.X_trainT.y_train\n",
    "        w = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "\n",
    "        # Predict target values on the test set\n",
    "        y_result = X.dot(w)\n",
    "\n",
    "        #  Manually compute root mean square error on the test set predictions\n",
    "        rmse = 0\n",
    "        mse = np.square(np.subtract(y,y_result)).mean() \n",
    "\n",
    "        rmse = math.sqrt(mse)\n",
    "        \n",
    "        intermediate_solut=(subset,rmse)\n",
    "        results.append(intermediate_solut)\n",
    "\n",
    "    return results\n",
    "\n",
    "results= manual_linear_regression_subset(housing, test_size= 0.1, seed= 42)\n",
    "resultss=dict()\n",
    "for res in results:\n",
    "    res=list(res)\n",
    "    key=res[-1]\n",
    "    resultss[key]=res[0]\n",
    "final_results=dict(sorted(resultss.items()))\n",
    "print(\"The features subset\", list(final_results.values())[0], \"results in a Linear Regression model with the lowest MSE.\")\n",
    "    \n",
    "\n",
    "# from solution import ....\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be MSEs and feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIelNBj9IdbO"
   },
   "source": [
    "### 2.1.2 <font color=\"red\">To Do</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.62963593 44.5770537  62.48813129]\n",
      "     MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  0.247805 -0.039065  0.560200   0.470478   -0.145947 -0.015506  0.423629   \n",
      "1  0.398949 -0.174391  0.315956   0.207376    0.118776  0.002345 -0.526459   \n",
      "2 -0.500166 -0.221106  0.265855   0.475035    0.099607  0.007205 -0.088686   \n",
      "\n",
      "   Longitude    target  \n",
      "0  -0.408498  0.184442  \n",
      "1   0.501717  0.352991  \n",
      "2   0.243144 -0.572408  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results_sorted=dict(sorted(results.items()))\\nsorted_values = sorted(results_sorted.values())\\nsorted_dict = {}\\n\\nfor i in sorted_values:\\n    for k in results_sorted.keys():\\n        if results_sorted[k] == i:\\n            sorted_dict[k] = results_sorted[k]'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Perform PCA to 3 dimensions (components)\n",
    "#and fit a Linear Regression using these 3 features.\n",
    "#Report the 3 features selected by PCA and the MSE. (0.5 points)\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "input_data = housing.data\n",
    "output_data = housing.target\n",
    "feature_names = housing.feature_names\n",
    "housing_df = pd.DataFrame(input_data,columns=feature_names)\n",
    "housing_df['target'] = output_data\n",
    "\n",
    "def pca(dataset_df):\n",
    "    \n",
    "    # standardization \n",
    "    scaler=StandardScaler()\n",
    "    scaler.fit(dataset_df) \n",
    "    scaled_data=scaler.transform(dataset_df)\n",
    "    \n",
    "    # pca step with scaled data\n",
    "    pca=PCA(n_components=3)\n",
    "    pca.fit(scaled_data)\n",
    "    x_pca=pca.transform(scaled_data)\n",
    "    print(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
    "    from sklearn import preprocessing\n",
    "    scaled_data = pd.DataFrame(preprocessing.scale(dataset_df),columns = dataset_df.columns)\n",
    "    print(pd.DataFrame(pca.components_,columns=scaled_data.columns))#,index = ['PC-1','PC-2','PC-3']))\n",
    "    \n",
    "    # Reconstruct the scaled data\n",
    "    scaled_data_r=pca.inverse_transform(x_pca) # still scaled\n",
    "    # Reconstruct the original data\n",
    "    data_r=scaler.inverse_transform(scaled_data_r) #back to original\n",
    "    \n",
    "    reconstr_error_scaleddata = np.square(np.subtract(scaled_data,scaled_data_r)).mean()\n",
    "    reconstr_error_original = np.square(np.subtract(dataset_df, data_r)).mean()\n",
    "    \n",
    "    return reconstr_error_original\n",
    "\n",
    "results=pca(housing_df)\n",
    "\n",
    "# calculate correlation of features--> the highest 3 = input for linear regression \n",
    "\n",
    "\"\"\"results_sorted=dict(sorted(results.items()))\n",
    "sorted_values = sorted(results_sorted.values())\n",
    "sorted_dict = {}\n",
    "\n",
    "for i in sorted_values:\n",
    "    for k in results_sorted.keys():\n",
    "        if results_sorted[k] == i:\n",
    "            sorted_dict[k] = results_sorted[k]\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# from solution import ....\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be MSE and feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_DByZGSdOEg"
   },
   "source": [
    "### 2.1.3 <font color=\"red\">To Do</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-pS81_N4RJF"
   },
   "source": [
    "## 2.2 Dimensions $8 \\rightarrow 1$ (1.5 points)\n",
    "\n",
    "Having to visualize the data across multiple dimensions can be cumbsersome. Let's perform the same task as in 2.1 but this time consider only one feature (both a subset of the 8 features and PCA with 1 component). This way it will be easier to visualize the relationaship of your predictive and target variables. Of course, you still want to select the feature that will result in the best performing model.\n",
    "\n",
    "Your output should include:\n",
    "\n",
    "1.   The respective MSEs (only the lowest MSE for the subset is fine.)\n",
    "2.   Make [a scatter plot](https://en.wikipedia.org/wiki/Scatter_plot) of the data with prices on the $y$ axis and the single feature/principal component on the $x$ axis. In the plot, also include a line as defined by the Linear Regression. (Make sure you don't forget to set the correct slope and y-intercept (constant))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1636738522928,
     "user": {
      "displayName": "Vilém Zouhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjqsASJYVkME5c30t4iFtrEDz__wtyoHv63qsWl4A=s64",
      "userId": "15042577288802340958"
     },
     "user_tz": -60
    },
    "id": "rbBzhK6f42J2",
    "outputId": "b0ef4961-7287-493b-e8b8-aacd7dd83209"
   },
   "outputs": [],
   "source": [
    "# from solution import ....\n",
    "\n",
    "# import your function from your .py file here and run this cell when you're done!\n",
    "# outputs should be the MSEs, feauture names, and plots (make sure to label your plots!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxMm8kFf_DYF"
   },
   "source": [
    "## 3 Manual Regression (4 points)\n",
    "\n",
    "Now that you are a bit more familiar with the data and the features. This exercise aims to develop some intuition behind regressions by manually adjusting the parameters (coefficients and intercept) in the model. The functions below all perform regression (predicting a real value) but they are far from perfect. Your goal is to improve the four functions from the initial ones in the `code cells` as follows:\n",
    "\n",
    "1. `hand_base` should serve as a baseline. The constraint is that it should only return a *single (constant) number* for all values. In other words, this is a model with no adjustable parameters. However, for the dataset there exists a unique value that minimizes the Mean Squared Error (MSE). Which one is it? (0.5 points) \n",
    "2. `hand_linear` should be a reasonable *linear* function that utilizes the input feature(s). Note that it should be strictly linear, that is in the form $\\sum \\lambda_i x_i+\\lambda_{const}$ where $\\lambda_k$ and $\\lambda_{const}$ are the coefficients and intercept that you can estimate from the given data by *trial and error*. Your estimates should be reasonable, i.e. definitely better than `hand_base`. Do this exercise before proceeding to the next function where you will obtain the coefficients and intercepts from fitting a Linear Regression model using *sklearn*. We will award full points based on any justified solution that's better than `hand_base`. Make sure that you read what the features mean and argue why you chose the specific formula. (1 point) (Note: we are *not* asking you to compute the coefficients and intercept, but rather play around with adjusting the coefficients and intercept manually to arrive at your best estimate.)\n",
    "3. `auto_linear`, obtain the coefficients and intercept from fitting a Linear Regression model using `sklearn`.\n",
    "(Consult [sklearn Linear Regression Documention](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on how to obtain the model's coefficients and intercept.) (1 point)\n",
    "\n",
    "4. `hand_complex` does not have any restriction on the content of the function. It can contain polynomial relationships (e.g. `x[0]*x[0]`), `if-else` statements, etc.) Now that you have both your hand crafted model and the one from `sklearn`, improve upon either of the models (or you can start with the parameters in the `auto_linear` model) so that the performance of the `hand_complex` is better than `auto_linear`.\n",
    "What are the disadvantages of this more complex approach apart from the difficulty of creating it? (Hint: think about unseen data.)\n",
    "\n",
    "Always comment on what led you to select the specific values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13EgQJtJdu7w"
   },
   "source": [
    "## 3 <font color=\"red\">To Do</font>\n",
    "\n",
    "Modify the functions in the `code cell` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1636912009088,
     "user": {
      "displayName": "Noonsky Noon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14548880660870589126"
     },
     "user_tz": 480
    },
    "id": "yo0nFWosRS_T",
    "outputId": "c0c3d622-4009-431d-a93a-dfad3701ef95"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Features\", housing.feature_names)\n",
    "\n",
    "def hand_base(_x):\n",
    "  # TODO: choose better single value\n",
    "  return 0\n",
    "\n",
    "def hand_linear(x):\n",
    "  # TODO: make me better but only linearly\n",
    "  return 2*x[0]-0.5*x[1]-0.1\n",
    "\n",
    "# TODO:\n",
    "# 1. Fit LinearRegression\n",
    "# 2. Report training MSE\n",
    "# 3. Examine the coefficients and intercept and use them for the `auto_linear` function\n",
    "# <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html>\n",
    "\n",
    "def auto_linear(x):\n",
    "  # TODO: use coefficients from your linear regression\n",
    "  return 2*x[0]-0.5*x[1]-0.1\n",
    "\n",
    "def hand_complex(x):\n",
    "  # TODO: make me better than the auto_linear function\n",
    "  if x[0] < 0.5:\n",
    "    return 0.1*x[1]\n",
    "  else:\n",
    "    return 0.2*x[1]\n",
    "\n",
    "print(f\"MSE Hand-Base: {mse(housing_y, [hand_base(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Hand-Linear: {mse(housing_y, [hand_linear(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Auto-LR: {mse(housing_y, [auto_linear(x) for x in housing_x]):.2f}\")\n",
    "print(f\"MSE Hand-Complex: {mse(housing_y, [hand_complex(x) for x in housing_x]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Pu7huYRyoW"
   },
   "source": [
    "# Bonus. Polynomial Regression and Overfitting (1 point):\n",
    "\n",
    "Find out how incorporating more features affects our model on the [California Housing Dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).\n",
    "\n",
    "1. Transform the feature space using polynomial features: <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html> (hint: make use of the Pipeline class) and run a regression model on top of it. Use degrees 1, 2, 3 and 4. \n",
    "\n",
    "2. Make a scatter plot with polynomial degree on the x-axis and training MSE on the y axis. What is an essential caveat to expanding the original feature space like this? (Hint: Think of unseen data again.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MdJPr9lDtUd"
   },
   "source": [
    "## Bonus: <font color=\"red\">To Do</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 2112,
     "status": "ok",
     "timestamp": 1636738647952,
     "user": {
      "displayName": "Vilém Zouhar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjqsASJYVkME5c30t4iFtrEDz__wtyoHv63qsWl4A=s64",
      "userId": "15042577288802340958"
     },
     "user_tz": -60
    },
    "id": "ccreCT1RDscQ",
    "outputId": "9282782f-00d0-4e8c-f891-823a434c6c85"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  \n",
    "# from solution import ....\n",
    "# import your function from your .py file here and run this cell when you're done!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "revised_assignment_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "d234eaca08f81c86c57ba3fab09f675b7f794e918ca74eabd6cec66c7f752238"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
