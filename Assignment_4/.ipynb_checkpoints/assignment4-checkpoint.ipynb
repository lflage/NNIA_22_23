{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT_S_io_85jk"
   },
   "source": [
    "# NNIA Assignment 4\n",
    "\n",
    "**DEADLINE: 7. 12. 2021 08:00 CET**  \n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx): \n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person: \n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required. \n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Calculus Revision (6 points)\n",
    "\n",
    "### 1.1 Critical Points (2 points)\n",
    "\n",
    "Consider the following functions: $f(x)=5x^2 +4x^3 −10$ and $g(x)=6(x−3)^3 +2$.\n",
    "\n",
    "1. Compute the first derivatives for both functions. (0.5 pts)\n",
    "2. Compute the second derivatives for both functions. (0.5 pts)\n",
    "3. Find all critical points for both functions (using first and second derivatives) and give their types (refer to lecture slides). Make sure to comment on how one determines the types of critical points. (1 pt)\n",
    "\n",
    "### 1.2 Local and Global (3 points)\n",
    "\n",
    "Some functions have more than one or two critical points such as function $h(x)=4x^2sin(x)+\\frac{e^x}{e^x+1} + x^3$.\n",
    "1. Compute the first derivative for $h$. (Hint: Use the Product and Quotient rules). (1 pt)\n",
    "2. Plot $h$ for the interval $x \\in (-4,5)$. Use an online resource like Wolfram Alpha or Desmos. (0.5 pt)\n",
    "3. Find the critical points from left to right and name them as $p_1, p_2,...p_n$ (no need for exact coordinates). For each point determine its type and if it is local or global. (0.5)\n",
    "4. Why can local minima cause optimization algorithms to fail? (0.5)\n",
    "\n",
    "### 1.3 Activation Functions (1.5 points)\n",
    "\n",
    "Three of the most commonly-used activation functions are the sigmoid function, hyperbolic tangent, and ReLU. The equations for these functions are provided below:\n",
    "\n",
    "* $\\sigma(x) = \\frac{1}{1+e^{-x}}$  \n",
    "\n",
    "* $\\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}$\n",
    "\n",
    "* $\\text{ReLU}(x) =  \\bigg\\{ \\begin{array}{ll} 0, & x < 0 \\\\ x, & x > 0 \\\\ \\end{array}$\n",
    "\n",
    "Again, using code or an online resource like Wolfram Alpha or Desmos, graph each function along with its derivative. Make sure to include the plots in your solution.\n",
    "1. Discuss the differences you observe. (1 sentence)\n",
    "2. What are the advantages and disadvantages of each? In particular, think about how the range of the function and the amplitude of the derivative would affect a network. (2 sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 <font color=\"red\">Done</font>\n",
    "\n",
    "#### 1.  \n",
    "\n",
    "$f(x)=5x^2 +4x^3 −10 $ \n",
    "\n",
    "Applying the power rule and the constant rule: \n",
    "\n",
    "$f'(x)= 2*5x^1 + 3*4x^2 = 10x + 12x^2$\n",
    "\n",
    "----------\n",
    "\n",
    "$g(x)=6(x−3)^3 +2$\n",
    "\n",
    "Factoring out the constant, applying the constant rule:\n",
    "\n",
    "$g'(x)=6*g'((x−3)^3) +g'(2) = 6*g'((x−3)^3) + 0$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$6*g'((x−3)^3) = 6* 3(x-3)^2*g'(x-3)= 18*(x-3)^2*g'(x-3)$\n",
    "\n",
    "Applying the sum rule:\n",
    "\n",
    "$18*(x-3)^2*g'(x-3) = 18(x-3)^2*(g'(x) + g'(-3)) = 18(x-3)^2*(1+0) = 18(x-3)^2$\n",
    "\n",
    "\n",
    "\n",
    "#### 2.\n",
    "\n",
    "$f'(x) = 10x + 12x^2$\n",
    "\n",
    "Sum rule, power rule:\n",
    "\n",
    "$f''(x) = 10 + 24x$\n",
    "\n",
    "----\n",
    "\n",
    "$g'(x) = 18(x-3)^2$\n",
    "\n",
    "Chain rule:\n",
    "\n",
    "$g''(x)= 36(x-3) * (1+0) = 36(x-3)$\n",
    "    \n",
    "#### 3.\n",
    "\n",
    "To find the critical points, we have to set the first derivative of the function to 0.\n",
    "\n",
    "$f'(x)= 10x + 12x^2$\n",
    "\n",
    "$ 10x + 12x^2 = 0 $   | Factoring\n",
    "\n",
    "$ 2x*(5 + 6x) = 0 $\n",
    "\n",
    "This will be 0 if either\n",
    "\n",
    "$ 5+6x = 0 $\n",
    "\n",
    "$ x_1 = -\\frac{5}{6} $\n",
    "\n",
    "Or\n",
    "\n",
    "$ 2x = 0 $\n",
    "\n",
    "$ x_2 = 0 $\n",
    "\n",
    "To find out if those are minima,maxima or saddle points, we can use the \"Second Derivative Test for Local Extrema\"\n",
    "\n",
    "$ f''(x_1) = 10 + 24*-\\frac{5}{6} = -10 $\n",
    "\n",
    "Since the second derivative at $x_1$ is negative, $x_1$ is a maximum.\n",
    "\n",
    "$ f''(x_2) = 10 + 24*0 = 10$\n",
    "\n",
    "Since the the second derivative at $x_2$  is positive, $x_2$ is a minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 <font color=\"red\">Done</font>\n",
    "\n",
    "1.   $h(x)=4x^2sin(x)+\\frac{e^x}{e^x+1} + x^3$\n",
    "\n",
    "Computing the first derivative:\n",
    "\n",
    "Sum rule:\n",
    "\n",
    "$h'(x) = h'(4x^2sin(x))+h'(\\frac{e^x}{e^x+1})+h'(x^3)$\n",
    "\n",
    "Product rule on first summand\n",
    "\n",
    "$h'(x) = 8x*sin(x)+4x^2*cos(x)+h'(\\frac{e^x}{e^x+1})+h'(x^3)$\n",
    "\n",
    "Quotient rule on second summand\n",
    "\n",
    "$h'(x) = 8x*sin(x)+4x^2*cos(x)+\\frac{e^x*(e^x+1)-e^{2x}}{(e^x+1)^2}+h'(x^3)$\n",
    "\n",
    "Fraction can be rewritten as\n",
    "\n",
    "$h'(x) = 8x*sin(x)+4x^2*cos(x)+\\frac{e^x}{(e^x+1)^2}+h'(x^3)$\n",
    "\n",
    "Power rule on third summand\n",
    "\n",
    "$h'(x) = 8x*sin(x)+4x^2*cos(x)+\\frac{e^x}{(e^x+1)^2}+ 3x^2$\n",
    "\n",
    "2.  \n",
    "\n",
    "![plot of h(x)](desmos_hx.png \"h(x)\")\n",
    "    \n",
    "3. $p_1 = -2.8$ global minimum\n",
    "\n",
    "$p_2 = 0$ saddle point\n",
    "\n",
    "$p_3 = 2.8$ global maximum\n",
    "\n",
    "$p_4 = 4.5$ local minimum\n",
    "\n",
    "4. If the learning rate is to small, the algorithm will get stuck in local minima, since it will move towards them but can't move out. If it takes only a small step out of the local minimum, it will move back into it with the next step since the (negative) gradient points it in that direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 <font color=\"red\">To Do</font>\n",
    "\n",
    "**Sigmoid Function:**\n",
    "Derivative: $ \\sigma'(x)=\\frac{e^x}{(e^x+1)^2}$ (in blue)\n",
    "\n",
    "![plot of sigma(x)](desmos_sigmoid.png \"sigma(x)\")\n",
    "\n",
    "**Tanh Function:**\n",
    "Derivative: $ \\tanh(x)'(x)=\\frac{4e^{2x}}{(e^{2x}+1)^2}$ (in blue)\n",
    "\n",
    "![plot of tanh(x)](desmos_tanh.png \"tanh(x)\")\n",
    "\n",
    "\n",
    "\n",
    "**Relu Function:**\n",
    "Derivative: $\\text{ReLU}(x) =  \\bigg\\{ \\begin{array}{ll} 0, & x < 0 \\\\ 1, & x > 0 \\\\ \\end{array}$ (in blue)\n",
    "\n",
    "![plot of relu(x)](desmos_relu.png \"relu(x)\")\n",
    "\n",
    "\n",
    "**Discussion**\n",
    "1. The shapes of sigmoid and tanh are similar, with the sigmoid squeezing values between 0 and 1, and the tanh between -1 and 1, while the ReLU function looks very different, having a linear increase for values > 0 and setting all values < 0 to 0.\n",
    "\n",
    "2. Both sigmoid and tanh give a smooth gradient and make clear predictions, whith the tanh being additionally zero-centric, but both suffer from the vanishing gradient problem. The ReLU in contrast doesn't suffer from this problem and is additionally computationally more effective, but it has the \"Dying ReLU\" problem which means, that for inputs close to zero or that are negative, the gradient will be zero and the network won't learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural Networks Hello World (4 points)\n",
    "\n",
    "Construct a neural network for the California Housing Prices dataset.\n",
    "It is composed of two linear layers with hidden dimensionality of $32$ and ReLU after the first layer: $L_{8\\rightarrow 32} \\circ \\text{ReLU} \\circ L_{32 \\rightarrow 1}$ where $L_{x\\rightarrow y}$ is a linear layer with $y$ hidden nodes (`nn.Linear(x, y)`).\n",
    "\n",
    "Split your data into dev and training sets (1000 training, 100 dev, discard rest) and optimize these networks with Adam with learning rate $10^{-3}$.\n",
    "Report MSE on both training and dev sets. You can use PyTorch built-in functions to solve this exercise. \n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- Run your training multiple times with different seeds because the results have quite high variance.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD (stochastic gradient descent). It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "######## Epoch 1 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 8.330294609069824\n",
      "Batch [20/100] | Loss: 4.21431827545166\n",
      "Batch [30/100] | Loss: 1.8796942234039307\n",
      "Batch [40/100] | Loss: 4.975155830383301\n",
      "Batch [50/100] | Loss: 3.323781967163086\n",
      "Batch [60/100] | Loss: 8.941208839416504\n",
      "Batch [70/100] | Loss: 4.585052490234375\n",
      "Batch [80/100] | Loss: 2.2229411602020264\n",
      "Batch [90/100] | Loss: 2.4397411346435547\n",
      "Batch [100/100] | Loss: 3.424111843109131\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 3.237697124481201\n",
      "Batch [2/10] | Loss 1.8997350931167603\n",
      "Batch [3/10] | Loss 1.5385866165161133\n",
      "Batch [4/10] | Loss 2.859257459640503\n",
      "Batch [5/10] | Loss 4.377992153167725\n",
      "Batch [6/10] | Loss 4.394412040710449\n",
      "Batch [7/10] | Loss 1.7434076070785522\n",
      "Batch [8/10] | Loss 5.1618266105651855\n",
      "Batch [9/10] | Loss 2.0286173820495605\n",
      "Batch [10/10] | Loss 3.290156126022339\n",
      "## Average loss 0.30531689524650574 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 2 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 4.239779949188232\n",
      "Batch [20/100] | Loss: 1.5719811916351318\n",
      "Batch [30/100] | Loss: 1.1295044422149658\n",
      "Batch [40/100] | Loss: 2.216887950897217\n",
      "Batch [50/100] | Loss: 1.060737133026123\n",
      "Batch [60/100] | Loss: 4.96840763092041\n",
      "Batch [70/100] | Loss: 2.5574452877044678\n",
      "Batch [80/100] | Loss: 0.6914421319961548\n",
      "Batch [90/100] | Loss: 0.8266436457633972\n",
      "Batch [100/100] | Loss: 1.291271686553955\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 1.5587583780288696\n",
      "Batch [2/10] | Loss 0.5542815327644348\n",
      "Batch [3/10] | Loss 0.4442771077156067\n",
      "Batch [4/10] | Loss 1.023813247680664\n",
      "Batch [5/10] | Loss 2.1681976318359375\n",
      "Batch [6/10] | Loss 2.4472079277038574\n",
      "Batch [7/10] | Loss 0.5556150078773499\n",
      "Batch [8/10] | Loss 2.3037586212158203\n",
      "Batch [9/10] | Loss 1.0393000841140747\n",
      "Batch [10/10] | Loss 1.5014755725860596\n",
      "## Average loss 0.13596683740615845 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 3 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 2.018724203109741\n",
      "Batch [20/100] | Loss: 0.386288583278656\n",
      "Batch [30/100] | Loss: 0.9159795641899109\n",
      "Batch [40/100] | Loss: 0.9935022592544556\n",
      "Batch [50/100] | Loss: 0.23802439868450165\n",
      "Batch [60/100] | Loss: 2.8255088329315186\n",
      "Batch [70/100] | Loss: 1.727678656578064\n",
      "Batch [80/100] | Loss: 0.34639424085617065\n",
      "Batch [90/100] | Loss: 0.40795502066612244\n",
      "Batch [100/100] | Loss: 0.7589138746261597\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 1.022580623626709\n",
      "Batch [2/10] | Loss 0.27442842721939087\n",
      "Batch [3/10] | Loss 0.34055593609809875\n",
      "Batch [4/10] | Loss 0.4512977600097656\n",
      "Batch [5/10] | Loss 1.2792072296142578\n",
      "Batch [6/10] | Loss 1.8617773056030273\n",
      "Batch [7/10] | Loss 0.8975845575332642\n",
      "Batch [8/10] | Loss 1.2854578495025635\n",
      "Batch [9/10] | Loss 0.8484814763069153\n",
      "Batch [10/10] | Loss 0.6886981129646301\n",
      "## Average loss 0.08950068801641464 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 4 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.566686749458313\n",
      "Batch [20/100] | Loss: 0.19049271941184998\n",
      "Batch [30/100] | Loss: 0.4851153790950775\n",
      "Batch [40/100] | Loss: 0.6710431575775146\n",
      "Batch [50/100] | Loss: 0.17326615750789642\n",
      "Batch [60/100] | Loss: 2.1651673316955566\n",
      "Batch [70/100] | Loss: 1.5626405477523804\n",
      "Batch [80/100] | Loss: 0.27713459730148315\n",
      "Batch [90/100] | Loss: 0.338346391916275\n",
      "Batch [100/100] | Loss: 0.6695494651794434\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.8840817213058472\n",
      "Batch [2/10] | Loss 0.23983116447925568\n",
      "Batch [3/10] | Loss 0.35830384492874146\n",
      "Batch [4/10] | Loss 0.27573519945144653\n",
      "Batch [5/10] | Loss 1.0246431827545166\n",
      "Batch [6/10] | Loss 1.7065473794937134\n",
      "Batch [7/10] | Loss 1.124207615852356\n",
      "Batch [8/10] | Loss 1.0946035385131836\n",
      "Batch [9/10] | Loss 0.7873883247375488\n",
      "Batch [10/10] | Loss 0.40003982186317444\n",
      "## Average loss 0.078953817486763 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 5 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.5530070066452026\n",
      "Batch [20/100] | Loss: 0.16315047442913055\n",
      "Batch [30/100] | Loss: 0.25393420457839966\n",
      "Batch [40/100] | Loss: 0.5645738840103149\n",
      "Batch [50/100] | Loss: 0.17509394884109497\n",
      "Batch [60/100] | Loss: 1.9534690380096436\n",
      "Batch [70/100] | Loss: 1.5186965465545654\n",
      "Batch [80/100] | Loss: 0.24773502349853516\n",
      "Batch [90/100] | Loss: 0.3144685924053192\n",
      "Batch [100/100] | Loss: 0.6210231781005859\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.8250298500061035\n",
      "Batch [2/10] | Loss 0.22353720664978027\n",
      "Batch [3/10] | Loss 0.3583827614784241\n",
      "Batch [4/10] | Loss 0.21990056335926056\n",
      "Batch [5/10] | Loss 0.9298825263977051\n",
      "Batch [6/10] | Loss 1.619702935218811\n",
      "Batch [7/10] | Loss 1.1066008806228638\n",
      "Batch [8/10] | Loss 1.0300997495651245\n",
      "Batch [9/10] | Loss 0.74700528383255\n",
      "Batch [10/10] | Loss 0.33481284976005554\n",
      "## Average loss 0.07394954562187195 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 6 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.4929338693618774\n",
      "Batch [20/100] | Loss: 0.14603467285633087\n",
      "Batch [30/100] | Loss: 0.19199195504188538\n",
      "Batch [40/100] | Loss: 0.5118064880371094\n",
      "Batch [50/100] | Loss: 0.16812561452388763\n",
      "Batch [60/100] | Loss: 1.8307230472564697\n",
      "Batch [70/100] | Loss: 1.4671152830123901\n",
      "Batch [80/100] | Loss: 0.2301289141178131\n",
      "Batch [90/100] | Loss: 0.30182167887687683\n",
      "Batch [100/100] | Loss: 0.5770113468170166\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.7808030247688293\n",
      "Batch [2/10] | Loss 0.21410802006721497\n",
      "Batch [3/10] | Loss 0.3521891236305237\n",
      "Batch [4/10] | Loss 0.20080995559692383\n",
      "Batch [5/10] | Loss 0.8535448908805847\n",
      "Batch [6/10] | Loss 1.547743558883667\n",
      "Batch [7/10] | Loss 1.0043199062347412\n",
      "Batch [8/10] | Loss 0.9687778353691101\n",
      "Batch [9/10] | Loss 0.7273844480514526\n",
      "Batch [10/10] | Loss 0.3148219585418701\n",
      "## Average loss 0.06964503228664398 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 7 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.3903928995132446\n",
      "Batch [20/100] | Loss: 0.12666817009449005\n",
      "Batch [30/100] | Loss: 0.16337569057941437\n",
      "Batch [40/100] | Loss: 0.4794260859489441\n",
      "Batch [50/100] | Loss: 0.16055507957935333\n",
      "Batch [60/100] | Loss: 1.711133599281311\n",
      "Batch [70/100] | Loss: 1.4152603149414062\n",
      "Batch [80/100] | Loss: 0.2179652750492096\n",
      "Batch [90/100] | Loss: 0.2949431538581848\n",
      "Batch [100/100] | Loss: 0.5389226078987122\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.7365543246269226\n",
      "Batch [2/10] | Loss 0.2081890106201172\n",
      "Batch [3/10] | Loss 0.34310829639434814\n",
      "Batch [4/10] | Loss 0.19336149096488953\n",
      "Batch [5/10] | Loss 0.7911120653152466\n",
      "Batch [6/10] | Loss 1.4769747257232666\n",
      "Batch [7/10] | Loss 0.8875546455383301\n",
      "Batch [8/10] | Loss 0.9124542474746704\n",
      "Batch [9/10] | Loss 0.7240076661109924\n",
      "Batch [10/10] | Loss 0.29427772760391235\n",
      "## Average loss 0.06567593663930893 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 8 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.2747101783752441\n",
      "Batch [20/100] | Loss: 0.11006565392017365\n",
      "Batch [30/100] | Loss: 0.14834533631801605\n",
      "Batch [40/100] | Loss: 0.45692047476768494\n",
      "Batch [50/100] | Loss: 0.15887299180030823\n",
      "Batch [60/100] | Loss: 1.603664755821228\n",
      "Batch [70/100] | Loss: 1.3615872859954834\n",
      "Batch [80/100] | Loss: 0.20684707164764404\n",
      "Batch [90/100] | Loss: 0.28629255294799805\n",
      "Batch [100/100] | Loss: 0.5094740986824036\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.7004546523094177\n",
      "Batch [2/10] | Loss 0.20257249474525452\n",
      "Batch [3/10] | Loss 0.3342815041542053\n",
      "Batch [4/10] | Loss 0.18750742077827454\n",
      "Batch [5/10] | Loss 0.7362553477287292\n",
      "Batch [6/10] | Loss 1.411538004875183\n",
      "Batch [7/10] | Loss 0.7844945788383484\n",
      "Batch [8/10] | Loss 0.8612476587295532\n",
      "Batch [9/10] | Loss 0.72050940990448\n",
      "Batch [10/10] | Loss 0.27838414907455444\n",
      "## Average loss 0.06217245012521744 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 9 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.1643654108047485\n",
      "Batch [20/100] | Loss: 0.09596645832061768\n",
      "Batch [30/100] | Loss: 0.142599418759346\n",
      "Batch [40/100] | Loss: 0.43883228302001953\n",
      "Batch [50/100] | Loss: 0.1622428148984909\n",
      "Batch [60/100] | Loss: 1.5131199359893799\n",
      "Batch [70/100] | Loss: 1.3093806505203247\n",
      "Batch [80/100] | Loss: 0.19768016040325165\n",
      "Batch [90/100] | Loss: 0.2821691930294037\n",
      "Batch [100/100] | Loss: 0.4897911548614502\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.6712106466293335\n",
      "Batch [2/10] | Loss 0.20327994227409363\n",
      "Batch [3/10] | Loss 0.3270316421985626\n",
      "Batch [4/10] | Loss 0.18131306767463684\n",
      "Batch [5/10] | Loss 0.6910662055015564\n",
      "Batch [6/10] | Loss 1.35255765914917\n",
      "Batch [7/10] | Loss 0.7068370580673218\n",
      "Batch [8/10] | Loss 0.8161662817001343\n",
      "Batch [9/10] | Loss 0.7191725969314575\n",
      "Batch [10/10] | Loss 0.261686235666275\n",
      "## Average loss 0.05930321291089058 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 10 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 1.0614200830459595\n",
      "Batch [20/100] | Loss: 0.08560533821582794\n",
      "Batch [30/100] | Loss: 0.14555984735488892\n",
      "Batch [40/100] | Loss: 0.42115315794944763\n",
      "Batch [50/100] | Loss: 0.168671652674675\n",
      "Batch [60/100] | Loss: 1.4384429454803467\n",
      "Batch [70/100] | Loss: 1.2637336254119873\n",
      "Batch [80/100] | Loss: 0.19085529446601868\n",
      "Batch [90/100] | Loss: 0.28167930245399475\n",
      "Batch [100/100] | Loss: 0.4788672924041748\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.6459287405014038\n",
      "Batch [2/10] | Loss 0.20501811802387238\n",
      "Batch [3/10] | Loss 0.3210904002189636\n",
      "Batch [4/10] | Loss 0.18075186014175415\n",
      "Batch [5/10] | Loss 0.6557931303977966\n",
      "Batch [6/10] | Loss 1.2996723651885986\n",
      "Batch [7/10] | Loss 0.6441341638565063\n",
      "Batch [8/10] | Loss 0.777769923210144\n",
      "Batch [9/10] | Loss 0.7231556177139282\n",
      "Batch [10/10] | Loss 0.2429865300655365\n",
      "## Average loss 0.056963011622428894 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 11 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.9732972383499146\n",
      "Batch [20/100] | Loss: 0.07729737460613251\n",
      "Batch [30/100] | Loss: 0.14719989895820618\n",
      "Batch [40/100] | Loss: 0.40476053953170776\n",
      "Batch [50/100] | Loss: 0.1799192875623703\n",
      "Batch [60/100] | Loss: 1.3784716129302979\n",
      "Batch [70/100] | Loss: 1.219534993171692\n",
      "Batch [80/100] | Loss: 0.18460464477539062\n",
      "Batch [90/100] | Loss: 0.28548258543014526\n",
      "Batch [100/100] | Loss: 0.46771159768104553\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.6251488327980042\n",
      "Batch [2/10] | Loss 0.20965759456157684\n",
      "Batch [3/10] | Loss 0.3167039752006531\n",
      "Batch [4/10] | Loss 0.18576082587242126\n",
      "Batch [5/10] | Loss 0.6301652193069458\n",
      "Batch [6/10] | Loss 1.2583726644515991\n",
      "Batch [7/10] | Loss 0.5961648225784302\n",
      "Batch [8/10] | Loss 0.75120609998703\n",
      "Batch [9/10] | Loss 0.7271782755851746\n",
      "Batch [10/10] | Loss 0.23237478733062744\n",
      "## Average loss 0.05532732605934143 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 12 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.8923999071121216\n",
      "Batch [20/100] | Loss: 0.0716259703040123\n",
      "Batch [30/100] | Loss: 0.15061156451702118\n",
      "Batch [40/100] | Loss: 0.38502979278564453\n",
      "Batch [50/100] | Loss: 0.19238033890724182\n",
      "Batch [60/100] | Loss: 1.3296949863433838\n",
      "Batch [70/100] | Loss: 1.1832969188690186\n",
      "Batch [80/100] | Loss: 0.1788042038679123\n",
      "Batch [90/100] | Loss: 0.2912055552005768\n",
      "Batch [100/100] | Loss: 0.46254587173461914\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.6087555289268494\n",
      "Batch [2/10] | Loss 0.2147325724363327\n",
      "Batch [3/10] | Loss 0.31247133016586304\n",
      "Batch [4/10] | Loss 0.19207429885864258\n",
      "Batch [5/10] | Loss 0.6068704724311829\n",
      "Batch [6/10] | Loss 1.2266812324523926\n",
      "Batch [7/10] | Loss 0.558167576789856\n",
      "Batch [8/10] | Loss 0.7308142185211182\n",
      "Batch [9/10] | Loss 0.7311684489250183\n",
      "Batch [10/10] | Loss 0.2261018007993698\n",
      "## Average loss 0.054078374058008194 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 13 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.8217126131057739\n",
      "Batch [20/100] | Loss: 0.06838841736316681\n",
      "Batch [30/100] | Loss: 0.15531188249588013\n",
      "Batch [40/100] | Loss: 0.3663117587566376\n",
      "Batch [50/100] | Loss: 0.20485052466392517\n",
      "Batch [60/100] | Loss: 1.2875821590423584\n",
      "Batch [70/100] | Loss: 1.1546885967254639\n",
      "Batch [80/100] | Loss: 0.17378929257392883\n",
      "Batch [90/100] | Loss: 0.2970595955848694\n",
      "Batch [100/100] | Loss: 0.4573812484741211\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5959609746932983\n",
      "Batch [2/10] | Loss 0.2210848331451416\n",
      "Batch [3/10] | Loss 0.308912068605423\n",
      "Batch [4/10] | Loss 0.19642533361911774\n",
      "Batch [5/10] | Loss 0.5913203954696655\n",
      "Batch [6/10] | Loss 1.2032945156097412\n",
      "Batch [7/10] | Loss 0.5289503335952759\n",
      "Batch [8/10] | Loss 0.7173289060592651\n",
      "Batch [9/10] | Loss 0.7389387488365173\n",
      "Batch [10/10] | Loss 0.21948924660682678\n",
      "## Average loss 0.05321705341339111 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 14 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.7629213333129883\n",
      "Batch [20/100] | Loss: 0.06761089712381363\n",
      "Batch [30/100] | Loss: 0.15831251442432404\n",
      "Batch [40/100] | Loss: 0.3485805094242096\n",
      "Batch [50/100] | Loss: 0.21647045016288757\n",
      "Batch [60/100] | Loss: 1.2497888803482056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [70/100] | Loss: 1.1304411888122559\n",
      "Batch [80/100] | Loss: 0.1689240038394928\n",
      "Batch [90/100] | Loss: 0.3035695254802704\n",
      "Batch [100/100] | Loss: 0.45725613832473755\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5842734575271606\n",
      "Batch [2/10] | Loss 0.22653993964195251\n",
      "Batch [3/10] | Loss 0.30774807929992676\n",
      "Batch [4/10] | Loss 0.2014981508255005\n",
      "Batch [5/10] | Loss 0.5779595971107483\n",
      "Batch [6/10] | Loss 1.190889596939087\n",
      "Batch [7/10] | Loss 0.5051663517951965\n",
      "Batch [8/10] | Loss 0.7020452618598938\n",
      "Batch [9/10] | Loss 0.740200400352478\n",
      "Batch [10/10] | Loss 0.21702083945274353\n",
      "## Average loss 0.05253342166543007 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 15 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.7174718379974365\n",
      "Batch [20/100] | Loss: 0.066422238945961\n",
      "Batch [30/100] | Loss: 0.16142363846302032\n",
      "Batch [40/100] | Loss: 0.3323652744293213\n",
      "Batch [50/100] | Loss: 0.22860634326934814\n",
      "Batch [60/100] | Loss: 1.2204258441925049\n",
      "Batch [70/100] | Loss: 1.1108853816986084\n",
      "Batch [80/100] | Loss: 0.16547931730747223\n",
      "Batch [90/100] | Loss: 0.30860501527786255\n",
      "Batch [100/100] | Loss: 0.4543936848640442\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5745314359664917\n",
      "Batch [2/10] | Loss 0.23216620087623596\n",
      "Batch [3/10] | Loss 0.3057509958744049\n",
      "Batch [4/10] | Loss 0.20349887013435364\n",
      "Batch [5/10] | Loss 0.5669869184494019\n",
      "Batch [6/10] | Loss 1.182003140449524\n",
      "Batch [7/10] | Loss 0.48421144485473633\n",
      "Batch [8/10] | Loss 0.6939913034439087\n",
      "Batch [9/10] | Loss 0.7402999997138977\n",
      "Batch [10/10] | Loss 0.2159155309200287\n",
      "## Average loss 0.051993560045957565 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 16 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.6809290051460266\n",
      "Batch [20/100] | Loss: 0.065299391746521\n",
      "Batch [30/100] | Loss: 0.16028930246829987\n",
      "Batch [40/100] | Loss: 0.31866753101348877\n",
      "Batch [50/100] | Loss: 0.23936375975608826\n",
      "Batch [60/100] | Loss: 1.1948978900909424\n",
      "Batch [70/100] | Loss: 1.0994136333465576\n",
      "Batch [80/100] | Loss: 0.1626356989145279\n",
      "Batch [90/100] | Loss: 0.3147139549255371\n",
      "Batch [100/100] | Loss: 0.4512001574039459\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5644532442092896\n",
      "Batch [2/10] | Loss 0.23853468894958496\n",
      "Batch [3/10] | Loss 0.30420592427253723\n",
      "Batch [4/10] | Loss 0.20444396138191223\n",
      "Batch [5/10] | Loss 0.5568746328353882\n",
      "Batch [6/10] | Loss 1.172368049621582\n",
      "Batch [7/10] | Loss 0.46681898832321167\n",
      "Batch [8/10] | Loss 0.6903694272041321\n",
      "Batch [9/10] | Loss 0.7383147478103638\n",
      "Batch [10/10] | Loss 0.21727967262268066\n",
      "## Average loss 0.05153663456439972 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 17 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.6500133275985718\n",
      "Batch [20/100] | Loss: 0.06372185796499252\n",
      "Batch [30/100] | Loss: 0.16391249001026154\n",
      "Batch [40/100] | Loss: 0.3068510591983795\n",
      "Batch [50/100] | Loss: 0.24922677874565125\n",
      "Batch [60/100] | Loss: 1.1737065315246582\n",
      "Batch [70/100] | Loss: 1.0886834859848022\n",
      "Batch [80/100] | Loss: 0.15958762168884277\n",
      "Batch [90/100] | Loss: 0.3204757571220398\n",
      "Batch [100/100] | Loss: 0.44838371872901917\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5563176870346069\n",
      "Batch [2/10] | Loss 0.24399113655090332\n",
      "Batch [3/10] | Loss 0.3025229573249817\n",
      "Batch [4/10] | Loss 0.20496602356433868\n",
      "Batch [5/10] | Loss 0.5483108758926392\n",
      "Batch [6/10] | Loss 1.1662499904632568\n",
      "Batch [7/10] | Loss 0.45383405685424805\n",
      "Batch [8/10] | Loss 0.6866899728775024\n",
      "Batch [9/10] | Loss 0.7372885346412659\n",
      "Batch [10/10] | Loss 0.2139197587966919\n",
      "## Average loss 0.051140908151865005 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 18 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.6246575117111206\n",
      "Batch [20/100] | Loss: 0.06213350221514702\n",
      "Batch [30/100] | Loss: 0.1662667691707611\n",
      "Batch [40/100] | Loss: 0.2962852120399475\n",
      "Batch [50/100] | Loss: 0.25823354721069336\n",
      "Batch [60/100] | Loss: 1.1547998189926147\n",
      "Batch [70/100] | Loss: 1.0790623426437378\n",
      "Batch [80/100] | Loss: 0.15690311789512634\n",
      "Batch [90/100] | Loss: 0.3269909918308258\n",
      "Batch [100/100] | Loss: 0.445295512676239\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5492437481880188\n",
      "Batch [2/10] | Loss 0.24934110045433044\n",
      "Batch [3/10] | Loss 0.30086207389831543\n",
      "Batch [4/10] | Loss 0.20657870173454285\n",
      "Batch [5/10] | Loss 0.5410422682762146\n",
      "Batch [6/10] | Loss 1.162301778793335\n",
      "Batch [7/10] | Loss 0.4393235146999359\n",
      "Batch [8/10] | Loss 0.67781662940979\n",
      "Batch [9/10] | Loss 0.7300244569778442\n",
      "Batch [10/10] | Loss 0.20853355526924133\n",
      "## Average loss 0.05065067857503891 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 19 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.6019850969314575\n",
      "Batch [20/100] | Loss: 0.061178773641586304\n",
      "Batch [30/100] | Loss: 0.16904154419898987\n",
      "Batch [40/100] | Loss: 0.2868652045726776\n",
      "Batch [50/100] | Loss: 0.2662113904953003\n",
      "Batch [60/100] | Loss: 1.1408331394195557\n",
      "Batch [70/100] | Loss: 1.0692672729492188\n",
      "Batch [80/100] | Loss: 0.15421906113624573\n",
      "Batch [90/100] | Loss: 0.3324495255947113\n",
      "Batch [100/100] | Loss: 0.4385007917881012\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.5427728891372681\n",
      "Batch [2/10] | Loss 0.25493669509887695\n",
      "Batch [3/10] | Loss 0.2988278269767761\n",
      "Batch [4/10] | Loss 0.20572224259376526\n",
      "Batch [5/10] | Loss 0.5367114543914795\n",
      "Batch [6/10] | Loss 1.1583404541015625\n",
      "Batch [7/10] | Loss 0.4289020001888275\n",
      "Batch [8/10] | Loss 0.6720287799835205\n",
      "Batch [9/10] | Loss 0.7231983542442322\n",
      "Batch [10/10] | Loss 0.20550808310508728\n",
      "## Average loss 0.050269488245248795 ##\n",
      "-----------------------------------------\n",
      "\n",
      "######## Epoch 20 ########\n",
      "-----------------------------------------\n",
      "---------- Training losses ----------\n",
      "Batch [10/100] | Loss: 0.582100510597229\n",
      "Batch [20/100] | Loss: 0.06148260831832886\n",
      "Batch [30/100] | Loss: 0.17164196074008942\n",
      "Batch [40/100] | Loss: 0.2775830626487732\n",
      "Batch [50/100] | Loss: 0.27199721336364746\n",
      "Batch [60/100] | Loss: 1.1239732503890991\n",
      "Batch [70/100] | Loss: 1.0605995655059814\n",
      "Batch [80/100] | Loss: 0.15146571397781372\n",
      "Batch [90/100] | Loss: 0.3366769254207611\n",
      "Batch [100/100] | Loss: 0.4354172646999359\n",
      "\n",
      "\n",
      "---------- Test losses ----------\n",
      "Batch [1/10] | Loss 0.535004198551178\n",
      "Batch [2/10] | Loss 0.25958842039108276\n",
      "Batch [3/10] | Loss 0.2975303530693054\n",
      "Batch [4/10] | Loss 0.2059643715620041\n",
      "Batch [5/10] | Loss 0.5320741534233093\n",
      "Batch [6/10] | Loss 1.1554107666015625\n",
      "Batch [7/10] | Loss 0.4226223826408386\n",
      "Batch [8/10] | Loss 0.6664206385612488\n",
      "Batch [9/10] | Loss 0.717397928237915\n",
      "Batch [10/10] | Loss 0.19981081783771515\n",
      "## Average loss 0.049918241798877716 ##\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# useful imports\n",
    "#from sklearn.datasets import fetch_california_housing\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "\n",
    "# DONE\n",
    "from Solution_A4 import run_training\n",
    "# your outputs should include Training MSE and Dev MSE.\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Learning Rate Effect on Performance (1 point)\n",
    "\n",
    "Run your neural network with different learning rates (scale logarithmically from $10^{-10}$ to $10^{0}$) and observe the effects on performance. Importantly keep the number of optimization steps (epochs) constant. Because the process is very random run the model multiple times (e.g. 3 times) and plot the results. Try to form hypothesis about the results which you got.\n",
    "\n",
    "Don't worry, the model training can be done on a CPU and is not computationally intensive."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
