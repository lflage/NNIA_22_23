{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT_S_io_85jk"
   },
   "source": [
    "# NNIA Assignment 4\n",
    "\n",
    "**DEADLINE: 7. 12. 2021 08:00 CET**  \n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx): \n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person: \n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required. \n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any data or cache files (e.g. `__pycache__`).\n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Calculus Revision (6 points)\n",
    "\n",
    "### 1.1 Critical Points (2 points)\n",
    "\n",
    "Consider the following functions: $f(x)=5x^2 +4x^3 −10$ and $g(x)=6(x−3)^3 +2$.\n",
    "\n",
    "1. Compute the first derivatives for both functions. (0.5 pts)\n",
    "2. Compute the second derivatives for both functions. (0.5 pts)\n",
    "3. Find all critical points for both functions (using first and second derivatives) and give their types (refer to lecture slides). Make sure to comment on how one determines the types of critical points. (1 pt)\n",
    "\n",
    "### 1.2 Local and Global (3 points)\n",
    "\n",
    "Some functions have more than one or two critical points such as function $h(x)=4x^2sin(x)+\\frac{e^x}{e^x+1} + x^3$.\n",
    "1. Compute the first derivative for $h$. (Hint: Use the Product and Quotient rules). (1 pt)\n",
    "2. Plot $h$ for the interval $x \\in (-4,5)$. Use an online resource like Wolfram Alpha or Desmos. (0.5 pt)\n",
    "3. Find the critical points from left to right and name them as $p_1, p_2,...p_n$ (no need for exact coordinates). For each point determine its type and if it is local or global. (0.5)\n",
    "4. Why can local minima cause optimization algorithms to fail? (0.5)\n",
    "\n",
    "### 1.3 Activation Functions (1.5 points)\n",
    "\n",
    "Three of the most commonly-used activation functions are the sigmoid function, hyperbolic tangent, and ReLU. The equations for these functions are provided below:\n",
    "\n",
    "* $\\sigma(x) = \\frac{1}{1+e^{-x}}$  \n",
    "\n",
    "* $\\tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1}$\n",
    "\n",
    "* $\\text{ReLU}(x) =  \\bigg\\{ \\begin{array}{ll} 0, & x < 0 \\\\ x, & x > 0 \\\\ \\end{array}$\n",
    "\n",
    "Again, using code or an online resource like Wolfram Alpha or Desmos, graph each function along with its derivative. Make sure to include the plots in your solution.\n",
    "1. Discuss the differences you observe. (1 sentence)\n",
    "2. What are the advantages and disadvantages of each? In particular, think about how the range of the function and the amplitude of the derivative would affect a network. (2 sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 <font color=\"red\">To Do</font>\n",
    "\n",
    "1.  \n",
    "\n",
    "2.  \n",
    "    \n",
    "3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 <font color=\"red\">To Do</font>\n",
    "\n",
    "1.   \n",
    "\n",
    "2.  \n",
    "    \n",
    "3. \n",
    "\n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 <font color=\"red\">To Do</font>\n",
    "\n",
    "**Sigmoid Function:**\n",
    "\n",
    "\n",
    "\n",
    "**Tanh Function:**\n",
    "\n",
    "\n",
    "\n",
    "**Relu Function:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Neural Networks Hello World (4 points)\n",
    "\n",
    "Construct a neural network for the California Housing Prices dataset.\n",
    "It is composed of two linear layers with hidden dimensionality of $32$ and ReLU after the first layer: $L_{8\\rightarrow 32} \\circ \\text{ReLU} \\circ L_{32 \\rightarrow 1}$ where $L_{x\\rightarrow y}$ is a linear layer with $y$ hidden nodes (`nn.Linear(x, y)`).\n",
    "\n",
    "Split your data into dev and training sets (1000 training, 100 dev, discard rest) and optimize these networks with Adam with learning rate $10^{-3}$.\n",
    "Report MSE on both training and dev sets. You can use PyTorch built-in functions to solve this exercise. \n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- Run your training multiple times with different seeds because the results have quite high variance.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD (stochastic gradient descent). It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(20640, 8)\n",
      "(20640,)\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([1000, 8])\n",
      "torch.Size([1000])\n",
      "torch.Size([100, 8])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# from your solution module import ...\n",
    "# your outputs should include Training MSE and Dev MSE.\n",
    "# useful imports\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import solution\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)\n",
    "\n",
    "\n",
    "# Slicing data\n",
    "## Train Set\n",
    "\n",
    "### Standardizing the Data\n",
    "my_scaler = solution.StandardScaler()\n",
    "scaled_data= my_scaler.fit_transform(X=housing.data, y=housing.target)\n",
    "\n",
    "print(type(scaled_data))\n",
    "\n",
    "### Slice\n",
    "data_train = torch.from_numpy(np.float32(scaled_data[:1000]))\n",
    "target_train = torch.from_numpy(np.float32(housing.target[:1000]))\n",
    "\n",
    "### Checking for shapes\n",
    "print(data_train.shape)\n",
    "print(target_train.shape)\n",
    "\n",
    "### train_set is a touple\n",
    "train_set = (data_train, target_train)\n",
    "\n",
    "## Dev Set\n",
    "### Slice\n",
    "data_dev = torch.from_numpy(np.float32(scaled_data[1000:1100]))\n",
    "target_dev = torch.from_numpy(np.float32(housing.target[1000:1100]))\n",
    "\n",
    "### checking for shapes\n",
    "print(data_dev.shape)\n",
    "print(target_dev.shape)\n",
    "dev_set = (data_dev,target_dev)\n",
    "\n",
    "# Loading the data into a Dataloader\n",
    "custom_train = solution.CustomDataset(train_set)\n",
    "train_dataloader = DataLoader(custom_train, batch_size=1)\n",
    "\n",
    "custom_train = solution.CustomDataset(dev_set)\n",
    "dev_dataloader = DataLoader(custom_train, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 22.774704  [    0/ 1000]\n",
      "loss: 5.473494  [  100/ 1000]\n",
      "loss: 0.542767  [  200/ 1000]\n",
      "loss: 0.569807  [  300/ 1000]\n",
      "loss: 5.587613  [  400/ 1000]\n",
      "loss: 1.217331  [  500/ 1000]\n",
      "loss: 2.540500  [  600/ 1000]\n",
      "loss: 9.004272  [  700/ 1000]\n",
      "loss: 0.596347  [  800/ 1000]\n",
      "loss: 1.426098  [  900/ 1000]\n",
      "test cummulative loss:  45.003742078351934\n",
      "Avg MSE loss : 0.450037 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 13.863303  [    0/ 1000]\n",
      "loss: 2.007368  [  100/ 1000]\n",
      "loss: 0.207153  [  200/ 1000]\n",
      "loss: 0.144726  [  300/ 1000]\n",
      "loss: 1.616895  [  400/ 1000]\n",
      "loss: 0.000013  [  500/ 1000]\n",
      "loss: 0.578180  [  600/ 1000]\n",
      "loss: 4.220693  [  700/ 1000]\n",
      "loss: 0.005167  [  800/ 1000]\n",
      "loss: 0.142471  [  900/ 1000]\n",
      "test cummulative loss:  81.12672439889866\n",
      "Avg MSE loss : 0.811267 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 6.391637  [    0/ 1000]\n",
      "loss: 0.628753  [  100/ 1000]\n",
      "loss: 1.398122  [  200/ 1000]\n",
      "loss: 0.807675  [  300/ 1000]\n",
      "loss: 0.417762  [  400/ 1000]\n",
      "loss: 0.114544  [  500/ 1000]\n",
      "loss: 0.155751  [  600/ 1000]\n",
      "loss: 1.933071  [  700/ 1000]\n",
      "loss: 0.027551  [  800/ 1000]\n",
      "loss: 0.001887  [  900/ 1000]\n",
      "test cummulative loss:  118.36576747137588\n",
      "Avg MSE loss : 1.183658 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.434997  [    0/ 1000]\n",
      "loss: 0.437475  [  100/ 1000]\n",
      "loss: 1.466764  [  200/ 1000]\n",
      "loss: 0.664597  [  300/ 1000]\n",
      "loss: 0.281480  [  400/ 1000]\n",
      "loss: 0.048821  [  500/ 1000]\n",
      "loss: 0.102948  [  600/ 1000]\n",
      "loss: 0.927993  [  700/ 1000]\n",
      "loss: 0.036526  [  800/ 1000]\n",
      "loss: 0.010922  [  900/ 1000]\n",
      "test cummulative loss:  117.65919937065337\n",
      "Avg MSE loss : 1.176592 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.070571  [    0/ 1000]\n",
      "loss: 0.478051  [  100/ 1000]\n",
      "loss: 1.165188  [  200/ 1000]\n",
      "loss: 0.391241  [  300/ 1000]\n",
      "loss: 0.284321  [  400/ 1000]\n",
      "loss: 0.003955  [  500/ 1000]\n",
      "loss: 0.100113  [  600/ 1000]\n",
      "loss: 0.423983  [  700/ 1000]\n",
      "loss: 0.028200  [  800/ 1000]\n",
      "loss: 0.036746  [  900/ 1000]\n",
      "test cummulative loss:  108.46079809236107\n",
      "Avg MSE loss : 1.084608 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.274298  [    0/ 1000]\n",
      "loss: 0.585716  [  100/ 1000]\n",
      "loss: 0.954060  [  200/ 1000]\n",
      "loss: 0.236087  [  300/ 1000]\n",
      "loss: 0.280800  [  400/ 1000]\n",
      "loss: 0.001714  [  500/ 1000]\n",
      "loss: 0.106700  [  600/ 1000]\n",
      "loss: 0.212288  [  700/ 1000]\n",
      "loss: 0.013095  [  800/ 1000]\n",
      "loss: 0.055886  [  900/ 1000]\n",
      "test cummulative loss:  99.16003039991483\n",
      "Avg MSE loss : 0.991600 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.845072  [    0/ 1000]\n",
      "loss: 0.679948  [  100/ 1000]\n",
      "loss: 0.847251  [  200/ 1000]\n",
      "loss: 0.165514  [  300/ 1000]\n",
      "loss: 0.255729  [  400/ 1000]\n",
      "loss: 0.009909  [  500/ 1000]\n",
      "loss: 0.122694  [  600/ 1000]\n",
      "loss: 0.135565  [  700/ 1000]\n",
      "loss: 0.005644  [  800/ 1000]\n",
      "loss: 0.064262  [  900/ 1000]\n",
      "test cummulative loss:  92.89243956111022\n",
      "Avg MSE loss : 0.928924 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.629630  [    0/ 1000]\n",
      "loss: 0.725790  [  100/ 1000]\n",
      "loss: 0.801776  [  200/ 1000]\n",
      "loss: 0.134777  [  300/ 1000]\n",
      "loss: 0.239614  [  400/ 1000]\n",
      "loss: 0.016664  [  500/ 1000]\n",
      "loss: 0.136356  [  600/ 1000]\n",
      "loss: 0.108813  [  700/ 1000]\n",
      "loss: 0.003077  [  800/ 1000]\n",
      "loss: 0.066341  [  900/ 1000]\n",
      "test cummulative loss:  89.31484507915593\n",
      "Avg MSE loss : 0.893148 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.520042  [    0/ 1000]\n",
      "loss: 0.735091  [  100/ 1000]\n",
      "loss: 0.784220  [  200/ 1000]\n",
      "loss: 0.121889  [  300/ 1000]\n",
      "loss: 0.232108  [  400/ 1000]\n",
      "loss: 0.021173  [  500/ 1000]\n",
      "loss: 0.144759  [  600/ 1000]\n",
      "loss: 0.099969  [  700/ 1000]\n",
      "loss: 0.002720  [  800/ 1000]\n",
      "loss: 0.065929  [  900/ 1000]\n",
      "test cummulative loss:  87.31281355620649\n",
      "Avg MSE loss : 0.873128 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.459629  [    0/ 1000]\n",
      "loss: 0.727157  [  100/ 1000]\n",
      "loss: 0.779241  [  200/ 1000]\n",
      "loss: 0.116532  [  300/ 1000]\n",
      "loss: 0.228485  [  400/ 1000]\n",
      "loss: 0.024235  [  500/ 1000]\n",
      "loss: 0.150695  [  600/ 1000]\n",
      "loss: 0.097322  [  700/ 1000]\n",
      "loss: 0.003205  [  800/ 1000]\n",
      "loss: 0.064154  [  900/ 1000]\n",
      "test cummulative loss:  86.22120994221405\n",
      "Avg MSE loss : 0.862212 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = solution.NeuralNetwork()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = .0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    solution.train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    solution.test_loop(dev_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2670559511364126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Learning Rate Effect on Performance (1 point)\n",
    "\n",
    "Run your neural network with different learning rates (scale logarithmically from $10^{-10}$ to $10^{0}$) and observe the effects on performance. Importantly keep the number of optimization steps (epochs) constant. Because the process is very random run the model multiple times (e.g. 3 times) and plot the results. Try to form hypothesis about the results which you got.\n",
    "\n",
    "Don't worry, the model training can be done on a CPU and is not computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace()\n",
    "\n",
    "\n",
    "model = solution.NeuralNetwork()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = .0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    solution.train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    solution.test_loop(dev_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nn_22_23')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "df4e0ccb632b6b26733995cf68ba48936afe98ab48de101e8a51d30964e64889"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
