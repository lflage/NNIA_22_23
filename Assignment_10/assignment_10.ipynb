{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NNIA Assignment 10\n",
        "\n",
        "**(Tentative) DEADLINE: 25. 01. 2023 08:00 CET**\n",
        "\n",
        "Submission more than 10 minutes past the deadline will **not** be graded!\n",
        "\n",
        "- Name & ID 1 (Teams username e.g. s8xxxxx):\n",
        "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
        "- Name & ID 3 (Teams username e.g. s8xxxxx):\n",
        "- Hours of work per person:\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
        "\n",
        "* Assignments are to be submitted in a **team of 2 or 3**.\n",
        "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
        "* Make sure you appropriately comment your code wherever required.\n",
        "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
        "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
        "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
        "* Only **one member** of the group should make the submisssion.\n",
        "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
      ],
      "metadata": {
        "id": "z5eYeUETpzBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Theory Review (1 point)\n",
        "\n",
        "Please consult the deeplearning book (namely [chapter 10](https://www.deeplearningbook.org/contents/rnn.html)), the lecture slides, or other resources to answer the following questions.\n",
        "\n",
        "Please note that some of these may not be covered in the lectures or may not be reached in this week's lecture. However, the deadline for this assignment is later than usual so you will have heard two lectures before the assignment is due.\n",
        "\n",
        "1. Explain what the teacher forcing training approach is like. (0.5 pts)\n",
        "1. Explain what problem the LSTM type of recurrent network and what each of the gates it uses addresses (0.5 pts)"
      ],
      "metadata": {
        "id": "IV6hn9TLr6Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 RNN (5 pts)\n",
        "\n",
        "The English alphabet has 26 letters. This is not the case for Hungarian(and many more European languages), which has many more extra letters, namely those with an acute accent (A) : `áéíóú`, those with an umlaut (U): `öü` and those with a double acute accent (D):  `őű`.\n",
        "Imagine you've been tasked by a telecommunication company to come up with a system to automatically add all the accents correctly given a word without accents.\n",
        "Each line in the input contains two words: with and without accents.\n",
        "We provide the train/dev split for you (in separate files).  \n",
        "  \n",
        "Build a character-level recurrent neural network (many (n) to many (n)) that classifies the correct accent (or no accent) for each letter of an input word.  E.g. for the word `ölében` with the accents removed: `oleben -> UNANNN`, where N refers to no accent.  \n",
        "  \n",
        "You can use non-vanilla RNN cells (such as LSTM or GRU) as well as any other tricks you know. A classification FFNN should follow the recurrent part. **Report train and dev results.**  \n",
        "You do not necessarily need a GPU for this task, but feel free to use Google Colab. Either way, we recommend that you work with a subset of the data first for faster development.\n",
        "\n",
        "**NOTE**: You should be able to generate the \"accented\" string using the output of your network. We recommend you use each type of accent (or lack thereof) as a possible output class. Once your model can predict these classes, use the predictions to apply \"edits\" to the unaccented string."
      ],
      "metadata": {
        "id": "YPwrzUtDpuVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vE0ITEhvp_Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qfeIsZqXqBPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train your model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "S6ro9NZtqCyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Report your results"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RADIc6ZdqFCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Introduction to HuggingFace and Pre-trained models (4 points)\n",
        "\n",
        "Now that you've worked with RNNs in their raw form, we will move on to using a *hub* of models. Well, what does that even mean?\n",
        "\n",
        "[HuggingFace](https://huggingface.co/) (HF) has taken it upon themselves to build a hub for models, datasets, metrics, etc. This basically means that dataset, metrics and models developed and trained by big labs are readily accessible through just a few lines of code.\n",
        "\n",
        "In this exercise your task will be to:\n",
        "\n",
        "1. Download a dataset from the HuggingFace repositories\n",
        "1. Download a model from the hub\n",
        "1. Train/fine-tune the aforementioned model\n",
        "1. Test your performance on the dataset\n",
        "\n",
        "We encourage you to look at the documentation and brief introductions to HuggingFace [here](https://huggingface.co/docs/transformers/index) and a quick tour of the library and hub [here](https://huggingface.co/docs/transformers/quicktour)."
      ],
      "metadata": {
        "id": "lcmPGsS52X2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we begin, we must install our datasets and transformers libraries\n",
        "!pip install -q datasets==2.8.0 transformers==4.25.1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HUPzc7466m_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Downloading the relevant dataset (0.5 points)\n",
        "\n",
        "For this task, we will be addressing yet another NLP task. A quite common one at that: sentiment analysis/classification.\n",
        "\n",
        "There are various datasets already pre-built for a task as popular as this. We will be using the Rotten Tomatoes dataset in particular [the one hosted here](https://huggingface.co/datasets/rotten_tomatoes).\n",
        "\n",
        "The link above leads you to the dataset page on HF. This page contains a lot of valuable information:\n",
        "\n",
        "1. A brief summary of the dataset. From this we learn how many datapoints we have for this classification task. Furthermore we learn where/how the data was collected as we are pointed to a publication.\n",
        "1. Information on the data structure. Here we learn how the data looks in its raw form, how much space it takes, etc. Importantly we also learn what default splits are included in the dataset.\n",
        "1. Additional information on curation processes, social impact, etc.\n",
        "\n",
        "Next you will be tasked to download the dataset and show 5 random examples (along with their labels)."
      ],
      "metadata": {
        "id": "cDElR7kO4W9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Download the dataset. Remember to import the necessary functions.\n",
        "# HINT: Check the HF quick tour and/or documentation pages\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "NjNUyk_a6ZaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Show 5 random examples for each split of the dataset. Use a seed of `42`"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "tZsenuzD7sIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully, after seeing a sample of the reviews we will be trying to classify, you already get a feeling for how hard this task could be."
      ],
      "metadata": {
        "id": "i1yTIkuQDE80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Downloading our relevant model (1 point)\n",
        "\n",
        "Here, we task you with downloading a model from the HF hub. The relevant model we will be working with is the RoBERTa model.\n",
        "\n",
        "[[model card]](https://huggingface.co/roberta-base) [[paper]](https://arxiv.org/abs/1907.11692) [[blog post]](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n",
        "\n",
        "You will learn more about this type of models in the next lecture. What you need to know right now is that RoBERTa is a powerful model built ontop of a transformer architecture. It is the result of a lot of progress in NLP and new architecture proposals and pre-training strategies.\n",
        "\n",
        "#### A brief note on transfer learning\n",
        "\n",
        "But **what is pre-training**? In Deep Learning for NLP, a paradigm shift happened around 2017/2018, where researchers went from training models from scratch for each downstream task to pre-training models on a general, usually self-supervised, task. After this pre-training was done (on massive amounts of data), the resulting model was then simply *fine-tuned* on the final downstream task.\n",
        "\n",
        "Pre-training is then simply this training phase on a very broad and general task that is usually not immediately applicable to solving a problem.\n",
        "\n",
        "**Why does this work?** Not everything about transfer learning is well-understood. The intuition behind it is that if you learn a general task, adaptaing your knowledge to more specific tasks should not take as much effort as learning from scratch. Furthermore, you can leverage the more general knowledge when adapting to a more specific task and end up performing better.\n",
        "\n",
        "For this task you are asked to download the RoBERTa model from the HF hub, both in its raw form (i.e. no pre-training) and in its pre-trained weights (i.e. the resulting weights learned from the pre-training are automatically loaded for you).\n",
        "\n",
        "### The task\n",
        "\n",
        "* Load the model architecture without any pre-trained weights loaded\n",
        "* Load the model with the pre-trained weights\n",
        "\n",
        "Note, our task, as we saw in the previous point, is a sequence classification, i.e. we assign a class to a whole sequence and not e.g. a label for each token.\n",
        "For this reason, you should load the model in its [SequenceClassification](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaForSequenceClassification) configuration."
      ],
      "metadata": {
        "id": "E0ZAHETs4W_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will load the tokenizer for you\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "P_fMsj8yE9T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Download the RoBERTa model from the HF hub *without* pretrained weights\n",
        "# HINT: Check the documentation here https://huggingface.co/docs/transformers/model_doc/roberta\n",
        "\n",
        "# TODO: model without pretrained weights\n",
        "\n",
        "# TODO: Download RoBERTa model from HF hub *with* the pretrained weights loaded"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "BxlxnMaD2qaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Training your models! (2.5 points)\n",
        "\n",
        "Now you will train your models.\n",
        "\n",
        "For this you have three alternative, two viable ones in the context of this course.\n",
        "\n",
        "HF offers a trainer class ([here is a quick guide on how to use it](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop)). Feel free to use this, as this will probably be the fastest way to get working code running.\n",
        "\n",
        "Alternatively, the models you have loaded both inherit from pytorch's `nn.Module`, meaning you can train them as you have trained models so far.\n",
        "\n",
        "1. Set up your training code\n",
        "1. Pick whatever hyper-parameters you deem necessary\n",
        "1. Use the validation split for the evaluation during training.\n",
        "1. Evaluate your final model with the test split.\n",
        "1. Report your findings for both models.\n",
        "1. Discuss what you expected to happen for each and how the end result compares to your expectations."
      ],
      "metadata": {
        "id": "3ABc7E-aLw6V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "hhx8RhB0FIjO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}