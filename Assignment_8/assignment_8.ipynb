{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5E36qAPf-8f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NNIA Assignment 8\n",
    "\n",
    "**DEADLINE: 11. 01. 2023 08:00 CET**\n",
    "Submission more than 10 minutes past the deadline will **not** be graded!\n",
    "\n",
    "- Name & ID 1 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 2 (Teams username e.g. s8xxxxx):\n",
    "- Name & ID 3 (Teams username e.g. s8xxxxx):\n",
    "- Hours of work per person:\n",
    "\n",
    "# Submission Instructions\n",
    "\n",
    "**IMPORTANT** Please make sure you read the following instructions carefully. If you are unclear about any part of the assignment, ask questions **before** the assignment deadline. All course-related questions can be addressed on the course **[Piazza Platform](https://piazza.com/class/kvc3vzhsvh55rt)**.\n",
    "\n",
    "* Assignments are to be submitted in a **team of 2 or 3**.\n",
    "* Please include your **names**, **ID's**, **Teams usernames**, and **approximate total time spent per person** at the beginning of the Notebook in the space provided\n",
    "* Make sure you appropriately comment your code wherever required.\n",
    "* Your final submission should contain this completed Jupyter Notebook, including the bonus question (if you attempt it), and any necessary Python files.\n",
    "* Do **not** submit any **data or cache files** (e.g. `__pycache__`, the dataset PyTorch downloads, etc.). \n",
    "* Upload the **zipped** folder (*.zip* is the only accepted extension) in **Teams**.\n",
    "* Only **one member** of the group should make the submisssion.\n",
    "* **Important** please name the submitted zip folder as: `Name1_id1_Name2_id2.zip`. The Jupyter Notebook should also be named: `Name1_id1_Name2_id2.ipynb`. This is **very important** for our internal organization epeatedly students fail to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv5YtUy8hrYv"
   },
   "source": [
    "## 1 SGD, Batch, Mini-Batch  (1.5 pts)\n",
    "\n",
    "Typically neural networks are large and are trained with millions of data points. It is thus often infeasible to compute the gradient $\\nabla_{\\theta} \\tilde J(\\theta)$ that requires the accumulation of the gradient over the entire training set. \n",
    "\n",
    "There are various online resources on Stochastic, Batch, and Minit-Gradient Descent methods in addition to what was covered during the lecture. Here are a few:\n",
    "\n",
    "- [Medium: Batch , Mini-Batch and Stochastic gradient descent](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461)\n",
    "- [DeepLearningAI: Batch vs Mini-Batch](https://youtu.be/4qJaSmvhxi8)\n",
    "\n",
    "**Discuss pros and cons of (1) stochastic (m=1), (2) batch (m = size of dataset) and (3) mini-batch gradient descent** (m is the number of points passed at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcPLBMij7xPK"
   },
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "### 1\n",
    "\n",
    "- Stochastic: \n",
    "\n",
    "- Batch: \n",
    "\n",
    "- Mini-Batch: \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZzXrUrUTMAT"
   },
   "source": [
    "## 2 Possible Problems (2.5 pts)\n",
    "\n",
    "1. One of the optimization challenges is ill-conditioning. To answer the following questions read [this article](https://medium.com/@shaikhz94/understanding-ill-conditioning-in-deep-neural-networks-2396d6fb0098) (6 min read). Answer the questions in your own words. (1.5 pts) \n",
    "  - Read part [8.2.1 Ill-Conditioning](https://www.deeplearningbook.org/contents/optimization.html) of the Deep Learning Book and explain why very small steps increase cost function when the Hessian matrix is ill-conditioned. Start from the equation of the second-order Taylor series expansion of the cost function.  \n",
    "  - In practice, how can we spot ill-conditioning?\n",
    "  - What can we do to solve the problem of ill-conditioning?\n",
    "\t\t\n",
    "2. Explain what the exploding gradient problem is and when it occurs. What can be done to solve the exploding gradient problem? (1 pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2DOmmMIEnsF"
   },
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "### 2\n",
    "\n",
    "1.  \n",
    "\n",
    "2. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKWOi_YlSqSM"
   },
   "source": [
    "## 3 Implementation (6 points)\n",
    "\n",
    "Now you will be implementing and testing different approaches to optimize the training of a neural network. Here you will be working with the [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#cifar10) dataset, which consists of images of airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Once again [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) will be used as the loss criterion.\n",
    "\n",
    "### 3.1 Early Stopping (2 points)\n",
    "\n",
    "Although not covered explicitly in the last lecture as an optimization strategy, early stopping can be used as both a regularization technique and an optimization technique.\n",
    "\n",
    "Early stopping is necessary since we are training and reducing empirical risk, thus our algorithms will not halt, they will simply be stopped once a convergence criteria has been met. Up until this point we have used a very crude strategy of simply limiting our learning to a number of epochs or steps.\n",
    "\n",
    "In this exercise, you are tasked with implementing a simple version of early stopping. The way to usually implement early stopping is to stop the training once a specific metric does not change more than an arbitrary value $\\varepsilon$ after a number of epochs (or steps) $n$, i.e.: $\\forall i \\in {1..n} \\,\\, \\left\\| M_i - M_{i-1} \\right\\| < \\varepsilon$ where $M_i$ is the value of the metric at epoch $i$.\n",
    "\n",
    "**Note**: The $n$ value above is usually called `patience` in common implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyNZRMIa6S_6"
   },
   "outputs": [],
   "source": [
    "!pip install -q torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-lqJ1_0cICv"
   },
   "outputs": [],
   "source": [
    "# Let's import some libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSUGCnRpntV_"
   },
   "source": [
    "#### CIFAR-10: Some notes about our data\n",
    "\n",
    "This dataset presents a multi-class classification task. More details can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Important details:\n",
    "\n",
    "* Each image is a color image of 32x32 pixels\n",
    "* There are 10 classes (laid out in the following code)\n",
    "* You should use the [MulticlassAccuracy](https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#multiclassaccuracy) from the `torcheval.metrics` library (already installed and imported for you).\n",
    "\n",
    "A quick overview of how to use metrics on pytorch can be found [here](https://torchmetrics.readthedocs.io/en/stable/pages/overview.html).\n",
    "\n",
    "The TL;DR of it all is to compute metrics per batch using the `update` method and computing a final metric value with `compute`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNs4XeBlb1Be"
   },
   "outputs": [],
   "source": [
    "# Let's load our data before getting into it\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLlHf5W1QJWV"
   },
   "outputs": [],
   "source": [
    "# A model is given for ease of reproducibility\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, lr=0.0001):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        self.learning_rate = lr\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM2MfDabZahy"
   },
   "outputs": [],
   "source": [
    "# Skeleton code \n",
    "class EarlyStopChecker:\n",
    "  def __init__(self,\n",
    "               delta: float,\n",
    "               patience: float,\n",
    "               lower_better: bool = False):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    delta: The max difference allowed\n",
    "    patience: Number of epochs to tolerate without significant changes\n",
    "    lower_better: The metric value is better the lower it is if `True` is passed\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    raise NotImplementedError\n",
    "  \n",
    "  def check_early_stop(self,\n",
    "                       metric_value: float) -> bool:\n",
    "    \"\"\"\n",
    "    A function which upon receiving the latest metric value, determines\n",
    "    whether training should be stopped (by returning `True`) or not \n",
    "    (by returning `False`)\n",
    "\n",
    "    Params:\n",
    "    metric_value: The value of the metric at this time step\n",
    "\n",
    "    Returns `True` if training should stop, `False` otherwise.\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FqVl2dF5brE"
   },
   "outputs": [],
   "source": [
    "# High learning rate so we can see the early stop effects quickly\n",
    "net = MyNetwork(lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGjLnLxTa82n"
   },
   "outputs": [],
   "source": [
    "# Training loop code\n",
    "epochs = 20\n",
    "early_stopper = EarlyStopChecker(0.1, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=net.learning_rate)\n",
    "metric = MulticlassAccuracy()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  running_loss = 0.0\n",
    "  for n_batch, mini_batch in (pbar := tqdm(enumerate(trainloader, 1),\n",
    "                                           total=len(trainloader))):\n",
    "    # Training code\n",
    "    inputs, labels = mini_batch\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Do optimization step\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print some statistics\n",
    "    running_loss += loss.item()\n",
    "    pbar.set_description(f\"Loss: {running_loss / n_batch:.4f}\")\n",
    "    pbar.refresh()\n",
    "\n",
    "  # TODO: compute metrics on the test/validation dataset\n",
    "  current_metric = ...\n",
    "\n",
    "  # TODO: Use `early_stopper.check_early_stop()` to check\n",
    "  # whether training should continue\n",
    "\n",
    "\n",
    "  print(f\"Accuracy for epoch #{epoch}: {current_metric.item()}\")\n",
    "\n",
    "  # Reset the metric in preparation for the next epoch\n",
    "  metric.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qvhYHD1cqJ5"
   },
   "source": [
    "### 3.2 Trying different optimizers (2 points)\n",
    "\n",
    "### 3.2.1 Using different optimizers in pytorch (1 point)\n",
    "\n",
    "It's time to try out the different optimizers covered in the lecture.\n",
    "\n",
    "Create a function that returns an optimizer object for each of the following:\n",
    "\n",
    "1. [SGD with Momentum](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html): note that you should set the momentum parameter to $0.9$. This is the value usually picked as the hyperparameter although other approaches to picking a value exist.\n",
    "1. [AdaGrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)\n",
    "1. [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)\n",
    "1. [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gb5pwtAd1-t"
   },
   "outputs": [],
   "source": [
    "# Set our seed for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbfKL7Jf8H1U"
   },
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name: str,\n",
    "                  model: nn.Module,\n",
    "                  learning_rate: float):\n",
    "  \"\"\"\n",
    "  Function that returns an optimizer object that will be used\n",
    "  for training\n",
    "\n",
    "  Params:\n",
    "  optimizer_name: This will indicate the type of optimizer to return\n",
    "  model: Model which has the parameters to be optimized\n",
    "  learning_rate: learning rate to be used\n",
    "  \"\"\"\n",
    "  # TODO: return the right object\n",
    "  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHcqFWcoRrj1"
   },
   "outputs": [],
   "source": [
    "# Usage:\n",
    "sgd_mom_optimizer = get_optimizer('momentum')\n",
    "adagrad_optimizer = get_optimizer('adagrad')\n",
    "rms_optimizer = get_optimizer('rmsprop')\n",
    "adam_optimizer = get_optimizer('adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbvJ-_sjeCes"
   },
   "source": [
    "#### 3.2.2 Discussion (1 point)\n",
    "\n",
    "What result do you expect for each of the learning optimizers? Motivate your answers while keeping them concise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQb3p7TlFt4d"
   },
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "#### 3.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsVwueL8eU6k"
   },
   "source": [
    "### 3.3 Analyze your results (2 points)\n",
    "\n",
    "Modify your training code from previous assignments (and the previous exercise if you wish to use early stopping) and experiment with the different optimization approaches on the CIFAR-10 dataset.\n",
    "\n",
    "Plot the training loss for each of the optimizers and discuss your results.\n",
    "\n",
    "How do they compare with your expectations from the previous exercise?\n",
    "\n",
    "You will be using the same dataset as above. You should also use the same model class (`MyNetwork`) for this exercise as in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzpF91NUGKOv"
   },
   "source": [
    "## <font color=\"red\">To Do</font>\n",
    "\n",
    "\n",
    "### 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7kV27rC6I_B"
   },
   "outputs": [],
   "source": [
    "# Train for each optimizer object\n",
    "for epoch in range(10):\n",
    "  for minibatch in trainloader:\n",
    "    # TODO: Training code here\n",
    "    # Train with each of the optimizers listed above\n",
    "    # Track your results (number of epochs until convergence, final loss, final accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-t4DsccEhdy4"
   },
   "outputs": [],
   "source": [
    "# Plot the resulting losses"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
